{"Data":{"GitHub":{"Issues":[{"Id":"354980619","IsPullRequest":false,"CreatedAt":"2018-08-29T03:22:28","Actor":"ericstj","Number":"763","RawContent":null,"Title":"ML.NET packages are missing third party notices","State":"open","Body":"Packages which include source / binaries that come from a third party need to be documented via a third party notices file in the root of the package.  The source repository should also contain a third party notices file in the root.\r\n\r\nFor example see https://github.com/dotnet/corefx/blob/master/THIRD-PARTY-NOTICES.TXT","Url":"https://github.com/dotnet/machinelearning/issues/763","RelatedDescription":"Open issue \"ML.NET packages are missing third party notices\" (#763)"},{"Id":"354940505","IsPullRequest":false,"CreatedAt":"2018-08-28T23:28:22","Actor":"jignparm","Number":"762","RawContent":null,"Title":"ONNX/Sonoma - upgrade to RS5 Lotus runtime and onnx 1.2 version support","State":"open","Body":"Upgrade to latest Lotus runtime and onnx model format","Url":"https://github.com/dotnet/machinelearning/issues/762","RelatedDescription":"Open issue \"ONNX/Sonoma - upgrade to RS5 Lotus runtime and onnx 1.2 version support\" (#762)"},{"Id":"354940288","IsPullRequest":false,"CreatedAt":"2018-08-28T23:27:08","Actor":"jignparm","Number":"761","RawContent":null,"Title":"ONNX/Sonoma - add API to retrieve shapes/dims of input and output layers","State":"open","Body":"Access shape info programatically instead of via user input","Url":"https://github.com/dotnet/machinelearning/issues/761","RelatedDescription":"Open issue \"ONNX/Sonoma - add API to retrieve shapes/dims of input and output layers\" (#761)"},{"Id":"354939964","IsPullRequest":false,"CreatedAt":"2018-08-28T23:25:21","Actor":"codemzs","Number":"760","RawContent":null,"Title":"Test for parquet loader because type system has changed","State":"open","Body":"We need to verify parquet loader works with the .NET standard type system.","Url":"https://github.com/dotnet/machinelearning/issues/760","RelatedDescription":"Open issue \"Test for parquet loader because type system has changed\" (#760)"},{"Id":"354885162","IsPullRequest":true,"CreatedAt":"2018-08-28T20:11:08","Actor":"Ivanidzo4ka","Number":"759","RawContent":null,"Title":"WIP Term transformer implementation","State":"open","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/759","RelatedDescription":"Open PR \"WIP Term transformer implementation\" (#759)"},{"Id":"354838907","IsPullRequest":false,"CreatedAt":"2018-08-28T17:55:27","Actor":"codemzs","Number":"758","RawContent":null,"Title":"Do not duplicate test output where there is no need to.","State":"open","Body":"Often times Debug and Release builds have identical test outputs but we create two seperate baselines for that. We should avoid doing this and instead have a common baseline.\r\n\r\nI believe @sfilipi was going to look into this?","Url":"https://github.com/dotnet/machinelearning/issues/758","RelatedDescription":"Open issue \"Do not duplicate test output where there is no need to.\" (#758)"},{"Id":"354811158","IsPullRequest":true,"CreatedAt":"2018-08-28T16:34:31","Actor":"codemzs","Number":"757","RawContent":null,"Title":"Add test for parquet loader.","State":"open","Body":"fixes #760 ","Url":"https://github.com/dotnet/machinelearning/pull/757","RelatedDescription":"Open PR \"Add test for parquet loader.\" (#757)"},{"Id":"354780092","IsPullRequest":false,"CreatedAt":"2018-08-28T15:18:41","Actor":"Zruty0","Number":"756","RawContent":null,"Title":"Move LearningPipeline API to a legacy namespace","State":"open","Body":"In order to free up `Microsoft.ML.*` names, we should move the auto-generated components into a different namespace, like `Microsoft.ML.Legacy`.\r\n\r\nWe also plan to deprecate the LearningPipeline API in 0.6 and remove it in 0.7.","Url":"https://github.com/dotnet/machinelearning/issues/756","RelatedDescription":"Open issue \"Move LearningPipeline API to a legacy namespace\" (#756)"},{"Id":"354454183","IsPullRequest":true,"CreatedAt":"2018-08-28T03:02:06","Actor":"codemzs","Number":"750","RawContent":null,"Title":"IDV test for DvTypes","State":"closed","Body":"This test ensures IDV file generated using DvTypes is parsed by the new type system that does not contain DvTypes.","Url":"https://github.com/dotnet/machinelearning/pull/750","RelatedDescription":"Closed or merged PR \"IDV test for DvTypes\" (#750)"},{"Id":"354528325","IsPullRequest":false,"CreatedAt":"2018-08-28T00:45:07","Actor":"Zruty0","Number":"755","RawContent":null,"Title":"SchemaShape's metadata to also be SchemaShape","State":"open","Body":"Currently, the 'metadata shape' associated with the column is a list of strings.\r\n\r\nIt is already somewhat problematic, because it doesn't have the associated types. \r\n\r\nWe should instead make it a `SchemaShape` on its own.","Url":"https://github.com/dotnet/machinelearning/issues/755","RelatedDescription":"Open issue \"SchemaShape's metadata to also be SchemaShape\" (#755)"},{"Id":"354511634","IsPullRequest":false,"CreatedAt":"2018-08-27T23:14:34","Actor":"Zruty0","Number":"754","RawContent":null,"Title":"New API for ML.NET","State":"open","Body":"We are creating a stable API that:\r\n- Uses parallel terminology with other well-known ML libraries (Spark, sklearn);\r\n- Takes advantage of strong types of .NET to shorten path to success;\r\n- Is going to be present from now till 1.0 and beyond;\r\n- Keeps simple ML scenarios concise;\r\n- Allows advanced ML scenarios: see #584 .\r\n\r\nTo that end, we are going to expose a selection of Estimators and Transformers (see #581 ) that cover existing transforms, learners and loaders. \r\n\r\nThis issue will be used to track the overall [project](https://github.com/dotnet/machinelearning/projects/9) status: what is planned to be done, what is done, etc.","Url":"https://github.com/dotnet/machinelearning/issues/754","RelatedDescription":"Open issue \"New API for ML.NET\" (#754)"},{"Id":"354504731","IsPullRequest":true,"CreatedAt":"2018-08-27T22:42:51","Actor":"Zruty0","Number":"753","RawContent":null,"Title":"Image transforms become Estimators","State":"open","Body":"Converted the following transforms to Estimators:\r\n- ImageLoader\r\n- ImageResizer\r\n- ImagePixelExtractor\r\n","Url":"https://github.com/dotnet/machinelearning/pull/753","RelatedDescription":"Open PR \"Image transforms become Estimators\" (#753)"},{"Id":"354472261","IsPullRequest":false,"CreatedAt":"2018-08-27T20:47:32","Actor":"petterton","Number":"752","RawContent":null,"Title":"Early stopping in LightGbm","State":"open","Body":"I am trying to implement a LightGbm classifier with early stopping, but I can not figure how to enter a validation set into the pipeline. I can see that EarlyStoppingRound can be set for LightGbm, so I suppose there should be a way to set a validation set. My current code:\r\n\r\n```\r\nvar pipeline = new LearningPipeline\r\n{\r\n    new TextLoader(TrainDataPath).CreateFrom<MyFeature>(useHeader: true),\r\n    new ColumnConcatenator(\"Features\", ... ),\r\n    new LightGbmBinaryClassifier\r\n    {\r\n        NumLeaves = 31, \r\n        NumBoostRound = 100, \r\n        EarlyStoppingRound = 10,\r\n    },\r\n};\r\n_model = pipeline.Train<MyFeature, MyPrediction>();\r\n\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/752","RelatedDescription":"Open issue \"Early stopping in LightGbm\" (#752)"},{"Id":"354467929","IsPullRequest":false,"CreatedAt":"2018-08-27T20:34:25","Actor":"zeahmed","Number":"751","RawContent":null,"Title":"Add checks on input between ML.Net and Tensorflow model.","State":"open","Body":"Add checking for input name, type, size and shape between ML.Net and Tensorflow model when the transform is being created so that user is able to pass correct input.","Url":"https://github.com/dotnet/machinelearning/issues/751","RelatedDescription":"Open issue \"Add checks on input between ML.Net and Tensorflow model.\" (#751)"},{"Id":"354385595","IsPullRequest":false,"CreatedAt":"2018-08-27T16:23:18","Actor":"lefig","Number":"749","RawContent":null,"Title":"Feature Selection","State":"open","Body":"\r\nHi all,\r\n\r\nFeature selection is an important part of any ML workflow. This is required to exclude too highly correlated labels from model fitting.  So rather than a process of trial and error I was hoping to select the best training model/evaluator based on running an initial feature selection action.\r\n\r\nJudging from the API documentation this may well be possible. For instance, we have the\r\n\r\nRankerEvaluator Class  \r\n\r\nHas anyone used this or any other aspect of the framework to help identify over fitting labels or best model to run?\r\n\r\nIn python I would manually do some correlation analysis and perhaps try something like TPOT to make a recommendation but would far prefer to stay in the world of C# and this excellent tool.\r\n\r\nBest regards\r\n\r\nFig ","Url":"https://github.com/dotnet/machinelearning/issues/749","RelatedDescription":"Open issue \"Feature Selection\" (#749)"},{"Id":"354380290","IsPullRequest":false,"CreatedAt":"2018-08-27T16:07:37","Actor":"yaeldekel","Number":"748","RawContent":null,"Title":"Documentation for TensorFlow transform","State":"open","Body":"","Url":"https://github.com/dotnet/machinelearning/issues/748","RelatedDescription":"Open issue \"Documentation for TensorFlow transform\" (#748)"},{"Id":"354379467","IsPullRequest":false,"CreatedAt":"2018-08-27T16:05:23","Actor":"yaeldekel","Number":"747","RawContent":null,"Title":"Investigate using text and sparse input in TensorFlow","State":"open","Body":"We should know how TF handles text inputs, and whether it supports sparse inputs.","Url":"https://github.com/dotnet/machinelearning/issues/747","RelatedDescription":"Open issue \"Investigate using text and sparse input in TensorFlow\" (#747)"},{"Id":"354379015","IsPullRequest":false,"CreatedAt":"2018-08-27T16:03:58","Actor":"yaeldekel","Number":"746","RawContent":null,"Title":"Enable specifying order of channels in Pixel Extractor","State":"open","Body":"TensorFlow models can have different ordering of the channels. ML.NET currently only supports specifying channel first/last, but not the order.","Url":"https://github.com/dotnet/machinelearning/issues/746","RelatedDescription":"Open issue \"Enable specifying order of channels in Pixel Extractor\" (#746)"},{"Id":"354377912","IsPullRequest":false,"CreatedAt":"2018-08-27T16:00:48","Actor":"yaeldekel","Number":"745","RawContent":null,"Title":"Handle additional types as input and output in TF models","State":"open","Body":"Initially only floats/doubles will be handled, and we should investigate which other types are commonly used in TF models.","Url":"https://github.com/dotnet/machinelearning/issues/745","RelatedDescription":"Open issue \"Handle additional types as input and output in TF models\" (#745)"},{"Id":"354279106","IsPullRequest":false,"CreatedAt":"2018-08-27T11:19:58","Actor":"rauhs","Number":"744","RawContent":null,"Title":"Calculating multiple TopK accuracies is slow/inefficient","State":"open","Body":"When calculating the top-k accuracy for multi-class classifiers the current evaluation code is pretty slow. Especially if multiple Top-K accuracies are desired (this will re-do a lot of the work unnecessarily).\r\n\r\nCurrent implementation will first sort (`N logN`):\r\n\r\n* https://github.com/dotnet/machinelearning/blob/f85e722fbd6b1710d104e85e6b3bcef4e593b5d2/src/Microsoft.ML.Data/Evaluators/MulticlassClassifierEvaluator.cs#L442-L446\r\n\r\nThen get the index:\r\n\r\n* https://github.com/dotnet/machinelearning/blob/f85e722fbd6b1710d104e85e6b3bcef4e593b5d2/src/Microsoft.ML.Data/Evaluators/MulticlassClassifierEvaluator.cs#L345-L350\r\n\r\nA more efficient algorithm would be to just calculate the rank (`O(N)` and no memory needed) of the correct label and keeping track of the seen ranks (0 being the best-case, correct prediction). Then the Top-k accuracy can be easily returned for any `k`. Possibly changing the API to returning a vector of top-k predictions.\r\n\r\nOne issue to discuss: What happens when the score is equal for multiple values? There would be a \"best-case\" and \"worst-case\" top-k accuracy.","Url":"https://github.com/dotnet/machinelearning/issues/744","RelatedDescription":"Open issue \"Calculating multiple TopK accuracies is slow/inefficient\" (#744)"},{"Id":"354267611","IsPullRequest":true,"CreatedAt":"2018-08-27T10:37:39","Actor":"rogancarr","Number":"743","RawContent":null,"Title":"Fixes for General Additive Models","State":"open","Body":"This PR addresses a number of issues with the `General Additive Model` (`GAM`) trainer. In particular, it addresses the issues with the `GAM Classifier` not fitting nor producing a probability, and adds support for validation pruning, summary text, and centering of the feature effects around a mean response (e.g. intercept).\r\n\r\nAdditionally, the PR addresses some minor issues in the codebase, like `GAMs` using copy-and-paste versions of `FastTree` code, unnecessarily `public` attributes, unused arguments, and splits the `BinaryClassifier` and `Regressor` into separate files.\r\n\r\nThe changes are as follows:\r\n1. Centered the response and added an intercept term; fixed corresponding issues with table lookups, sparse calculations, and exports. (#739)\r\n2. Added in a validation set and pruning based on the validation set, producing a final pruned graph at the end. (#740)\r\n  a. Switched to using `ScoreTrackers` to keep track of scores during boosted\r\n  b. Save boost iterations as individual graphs (n_features x n_iterations x n_boosts) for pruning.\r\n3. Fixed `GAM Classifier` to use a small learning rate (#741)\r\n  Updated the `FastTreeBinaryClassification` logistic loss gradient to take the `sigmoid` parameter as input: `GAM Binary Classifier` now uses unity; `FastTree Binary Classifier` uses the same default of `2*learning rate` (no change). Optionally, we can plumb this to the FastTree arguments if we show an experimental gain; We can also experiment to see if the `sigmoid` parameter gives gains for GAMs (i.e. by slowing learning even more).\r\n7. Added calibration to the `GamClassifier` to produce probabilities (#738).\r\n  This meant changing the various interfaces.\r\n8. Refactored `GAMRegressor` and `GAMClassifier` into their own files.\r\n  The one file for all GAM trainers had gotten a bit long, and this change is consistent with ML.NET tradition.\r\n9. Added tests to verify train loss and validation metrics (e.g. same value is produced by boosting and the lookup table).\r\n10. Added a Summary to the `GAMPredictor` to produce training statistics and the feature table (#742).\r\n11. Rewrote the core split finding routine to use the same code as FastTree to rely on FastTree unit tests\r\n  The `GAM` routines used a copy-and-paste of internal `FastTree` components. To fight entropy, these were refactored to use the same central calculation, such that it is verified and validated by the `FastTree` unit and end-to-end tests.\r\n12. Removed unused arguments.\r\n13. Allowed for weighted samples.\r\n\r\nFixes #738 \r\nFixes #739 \r\nFixes #740 \r\nFixes #741 \r\nFixes #742 \r\n","Url":"https://github.com/dotnet/machinelearning/pull/743","RelatedDescription":"Open PR \"Fixes for General Additive Models\" (#743)"},{"Id":"354261400","IsPullRequest":false,"CreatedAt":"2018-08-27T10:16:04","Actor":"rogancarr","Number":"742","RawContent":null,"Title":"General Additive Model (GAM) has no summary to extract features at runtime","State":"open","Body":"### Issue\r\n\r\nThe `Generalized Additive Model` (`GAM`) learner provides a save-as-text option [1], but no option to view the summary at runtime.\r\n\r\n[1] https://github.com/dotnet/machinelearning/blob/60ae981223e83b174ecaaf528bd51814a6b0835c/src/Microsoft.ML.FastTree/GamTrainer.cs#L820","Url":"https://github.com/dotnet/machinelearning/issues/742","RelatedDescription":"Open issue \"General Additive Model (GAM) has no summary to extract features at runtime\" (#742)"},{"Id":"354260495","IsPullRequest":false,"CreatedAt":"2018-08-27T10:12:56","Actor":"rogancarr","Number":"741","RawContent":null,"Title":"FastTree Gradient of Logistic Loss prohibits small learning rates","State":"open","Body":"### Issue\r\n\r\nThe gradient of the Logistic Loss implemented in `FastTree` uses the `LambdaRank`-style `sigmoid` parameter, set to the learning rate. This quashes the gradients for small learning rates. While this works well for classification tasks, when used by the `General Additive Model` (`GAM`) trainer, it prohibits learning with small learning rates. However, the `GAM` learning-by-boosting technique implemented here requires small learning rates to be stable.","Url":"https://github.com/dotnet/machinelearning/issues/741","RelatedDescription":"Open issue \"FastTree Gradient of Logistic Loss prohibits small learning rates\" (#741)"},{"Id":"354259243","IsPullRequest":false,"CreatedAt":"2018-08-27T10:08:38","Actor":"rogancarr","Number":"740","RawContent":null,"Title":"General Additive Models do not allow pruning with a validation set","State":"open","Body":"### Issue\r\n\r\nIn the standard formulation of GAMs by Rich Caruana's research group [[Lou et al. 2012](http://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf)], the features are pruned post-learning based on a validation set.\r\n\r\nIn ML.NET, pruning is not allowed.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/740","RelatedDescription":"Open issue \"General Additive Models do not allow pruning with a validation set\" (#740)"},{"Id":"354258708","IsPullRequest":false,"CreatedAt":"2018-08-27T10:06:50","Actor":"rogancarr","Number":"739","RawContent":null,"Title":"General Additive Model features are not centered after learning.","State":"open","Body":"### Issue\r\n\r\nIn the standard formulation of GAMs by Rich Caruana's research group [[Lou et al. 2012](http://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf)], the feature responses are centered around the average response in the training set, and an intercept term is added to capture the average response.\r\n\r\nIn the ML.NET formulation of GAMs, the feature values are not centered.\r\n\r\nThis is an issue when scoring terms with missing values, as those terms will then deviate from the average by an un-calibrated amount.","Url":"https://github.com/dotnet/machinelearning/issues/739","RelatedDescription":"Open issue \"General Additive Model features are not centered after learning.\" (#739)"},{"Id":"354257147","IsPullRequest":false,"CreatedAt":"2018-08-27T10:01:45","Actor":"rogancarr","Number":"738","RawContent":null,"Title":"General Additive Models (GAM) for Classification learn the logit, not class probability","State":"open","Body":"### System information\r\n\r\n_independent of system_\r\n\r\n### Issue\r\n\r\nThe `GAM Binary Classifier` in `ML.NET` learns to predict the *Logit*, but it is not calibrated to produce a probability. This goes against the standard pattern that we have for classifiers in `ML.NET`.\r\n\r\n### Source code / logs\r\n\r\nSee the `Create` function: https://github.com/dotnet/machinelearning/blob/60ae981223e83b174ecaaf528bd51814a6b0835c/src/Microsoft.ML.FastTree/GamTrainer.cs#L140\r\n","Url":"https://github.com/dotnet/machinelearning/issues/738","RelatedDescription":"Open issue \"General Additive Models (GAM) for Classification learn the logit, not class probability\" (#738)"},{"Id":"354243218","IsPullRequest":false,"CreatedAt":"2018-08-27T09:18:46","Actor":"lefig","Number":"737","RawContent":null,"Title":"Binary Classification Results","State":"open","Body":"Hi all,\r\n\r\nI have a quick question and would appreciate your thoughts regarding what could be a dubious score. I have been training/testing some bin classification models and the results look too promising. For instance,\r\n\r\nNot training a calibrator because it is not needed.\r\n*************************************************\r\n*       Metrics for Logistic Regression Binary\r\n*------------------------------------------------\r\n*       Accuracy: 1\r\n*       Entropy: 0.999316959634874\r\n*       Auc: 100.00%\r\n\r\nThis is not a software issue but rather a query. I would have expected accuracy to have been reported to decimal places (less than 1).\r\n\r\nThanks\r\n\r\nFig","Url":"https://github.com/dotnet/machinelearning/issues/737","RelatedDescription":"Open issue \"Binary Classification Results\" (#737)"},{"Id":"354199480","IsPullRequest":false,"CreatedAt":"2018-08-27T06:45:43","Actor":"rauhs","Number":"736","RawContent":null,"Title":"How to get auto tuned parameters to train final model","State":"open","Body":"Hi,\r\n\r\nafter training many models on time series data (multi class pred) I'd like to train a final model on all available data (including the test set).\r\n\r\nSpecifically I use SDCA multi class trainers and set some parameters while leaving other to \"auto-tune\".\r\n\r\nExample output:\r\n\r\n```\r\nAutomatically choosing a check frequency of 4.\r\nAuto-tuning parameters: maxIterations = 104.\r\nAuto-tuning parameters: L2 = 2.719283E-05.\r\nAuto-tuning parameters: L1Threshold (L1/L2) = 0.25.\r\nUsing best model from iteration 104.\r\n```\r\n\r\nHow would I train my final model? I'd like to set the iterations to 104 and the other hyper parameters to their respective values. Is there an API to get the estimated/used parameters?\r\n\r\nI can't just read stdout and manually train the model since I dynamically train ML models on our customer's servers.\r\n\r\nThanks for all work!","Url":"https://github.com/dotnet/machinelearning/issues/736","RelatedDescription":"Open issue \"How to get auto tuned parameters to train final model\" (#736)"},{"Id":"354177683","IsPullRequest":true,"CreatedAt":"2018-08-27T04:41:44","Actor":"adamsitnik","Number":"735","RawContent":null,"Title":"Support for custom metrics reported in the Benchmarks","State":"open","Body":"This PR enables two things: \r\n\r\n1. executing every benchmark in an isolated process\r\n2. reporting custom metrics per benchmark\r\n\r\nWhy should we run every benchmark in a separate process?\r\n\r\n1. Because most of ML.NET benchmarks allocate a lot of memory which affect GC Generation sizes and affects final results (GC is self-tuning if we run all the benchmarks in the same process GC won't be able to find a solution that is great for all of the benchmarks)\r\n2. Most of the ML.NET can have potential side effects. Example: running train benchmark after running predict benchmark in the same process can possibly affect the results. With new process per benchmark, we always start at the same place and have repeatable results.\r\n\r\nResults when running all the benchmarks in the same process:\r\n\r\n|                                          Type |              Method |         Mean |       Error |      StdDev |       Gen 0 |      Gen 1 |     Gen 2 |   Allocated |\r\n|---------------------------------------------- |-------------------- |-------------:|------------:|------------:|------------:|-----------:|----------:|------------:|\r\n|              KMeansAndLogisticRegressionBench |    TrainKMeansAndLR | 2,134.265 ms | 164.3370 ms | 189.2507 ms |  16000.0000 |  9000.0000 | 3000.0000 | 49949.23 KB |\r\n| StochasticDualCoordinateAscentClassifierBench |      TrainSentiment | 2,130.503 ms |  24.8173 ms |  23.2141 ms | 122000.0000 | 35000.0000 | 5000.0000 | 759772.8 KB |\r\n| StochasticDualCoordinateAscentClassifierBench |           TrainIris |   834.229 ms | 254.5284 ms | 293.1152 ms |   6000.0000 |  1000.0000 |         - | 12173.28 KB |\r\n| StochasticDualCoordinateAscentClassifierBench |         PredictIris |     2.472 ms |   0.1202 ms |   0.1384 ms |     35.1563 |    15.6250 |    3.9063 |   123.24 KB |\r\n| StochasticDualCoordinateAscentClassifierBench | PredictIrisBatchOf1 |     2.712 ms |   0.3276 ms |   0.3773 ms |     35.1563 |    15.6250 |    3.9063 |    123.2 KB |\r\n| StochasticDualCoordinateAscentClassifierBench | PredictIrisBatchOf2 |     2.370 ms |   0.1334 ms |   0.1482 ms |     35.1563 |    15.6250 |    3.9063 |   123.31 KB |\r\n| StochasticDualCoordinateAscentClassifierBench | PredictIrisBatchOf5 |     2.492 ms |   0.1678 ms |   0.1865 ms |     35.1563 |    15.6250 |    3.9063 |   123.61 KB |\r\n\r\nWhen running every benchmark in a dedicated process:\r\n\r\n|                                          Type |              Method |         Mean |       Error |      StdDev |       Gen 0 |      Gen 1 |     Gen 2 |    Allocated |\r\n|---------------------------------------------- |-------------------- |-------------:|------------:|------------:|------------:|-----------:|----------:|-------------:|\r\n|              KMeansAndLogisticRegressionBench |    TrainKMeansAndLR | 1,968.326 ms |  84.3827 ms |  97.1753 ms |  16000.0000 |  9000.0000 | 3000.0000 |  50027.36 KB |\r\n| StochasticDualCoordinateAscentClassifierBench |           TrainIris |   604.496 ms | 238.4849 ms | 274.6396 ms |  59000.0000 |  1000.0000 |         - |   76697.5 KB |\r\n| StochasticDualCoordinateAscentClassifierBench |      TrainSentiment | 1,829.670 ms |  10.9792 ms |  10.2699 ms | 123000.0000 | 35000.0000 | 6000.0000 | 759758.03 KB |\r\n| StochasticDualCoordinateAscentClassifierBench |         PredictIris |     1.895 ms |   0.0132 ms |   0.0111 ms |     35.1563 |    15.6250 |    3.9063 |    121.87 KB |\r\n| StochasticDualCoordinateAscentClassifierBench | PredictIrisBatchOf1 |     1.941 ms |   0.0145 ms |   0.0121 ms |     35.1563 |    15.6250 |    3.9063 |    119.94 KB |\r\n| StochasticDualCoordinateAscentClassifierBench | PredictIrisBatchOf2 |     1.960 ms |   0.0676 ms |   0.0751 ms |     35.1563 |    15.6250 |    3.9063 |    121.94 KB |\r\n| StochasticDualCoordinateAscentClassifierBench | PredictIrisBatchOf5 |     1.870 ms |   0.0043 ms |   0.0036 ms |     37.1094 |    17.5781 |    3.9063 |    120.35 KB |\r\n\r\nTo run every benchmark in a standalone, dedicated process BenchmarkDotNet needs to be able to create, build and run new executable.\r\n\r\nSo far it was not possible out of the box due to MSBuild limitation. When `Microsoft.ML.Benchmarks` references native assembly and the auto-generated BenchmarkDotNet project references `Microsoft.ML.Benchmarks` the native dependencies are not copied to the output folder of the auto-generated project with benchmarks. This is why I had to implement `ProjectGenerator` which does that for us.\r\n\r\n@eerhardt we had a conversation about making it possible for BenchmarkDotNet to compile ML.NET stuff a long time ago and the blocker was the native dependency.\r\n\r\nThe other thing are custom metrics. BenchmarkDotNet does not support it out of the box, I had to implement it. How it works:\r\n\r\n1.  If given type wants to report custom metrics it has to derive from `WithExtraMetrics` and implement `IEnumerable<Metric> GetMetrics()` method\r\n2. `WithExtraMetrics` after running the benchmarks prints the custom metrics to console in child process\r\n3. `ExtraMetricColumn` parses the output in parent process.\r\n\r\nSample results:\r\n\r\n|                                          Type |              Method |        Extra Metric |\r\n|---------------------------------------------- |-------------------- |--------------------:|\r\n|              KMeansAndLogisticRegressionBench |    TrainKMeansAndLR |                   - |\r\n| StochasticDualCoordinateAscentClassifierBench |           TrainIris |                   - |\r\n| StochasticDualCoordinateAscentClassifierBench |      TrainSentiment |                   - |\r\n| StochasticDualCoordinateAscentClassifierBench |         PredictIris | AccuracyMacro: 0.98 |\r\n| StochasticDualCoordinateAscentClassifierBench | PredictIrisBatchOf1 | AccuracyMacro: 0.98 |\r\n| StochasticDualCoordinateAscentClassifierBench | PredictIrisBatchOf2 | AccuracyMacro: 0.98 |\r\n| StochasticDualCoordinateAscentClassifierBench | PredictIrisBatchOf5 | AccuracyMacro: 0.98 |\r\n\r\nOther changes: so far the benchmarks were using `currentAssemblyLocation.Directory.Parent.Parent.Parent.Parent.FullName` to get the path to folder with input files. I believe it's better to reference them as links in `csproj` and \"copy to output directory if newer\". This solution is cleaner and more futureproof.\r\n\r\n/cc @eerhardt @danmosemsft @briancylui @KrzysztofCwalina  ","Url":"https://github.com/dotnet/machinelearning/pull/735","RelatedDescription":"Open PR \"Support for custom metrics reported in the Benchmarks\" (#735)"},{"Id":"354132651","IsPullRequest":true,"CreatedAt":"2018-08-26T21:59:47","Actor":"eerhardt","Number":"734","RawContent":null,"Title":"Convert ML.Sweeper usages of SubComponent to IComponentFactory.","State":"open","Body":"Moving all the SubComponent usages in the ML.Sweeper assembly to use IComponentFactory.\r\n\r\nNote: I logged https://github.com/dotnet/machinelearning/issues/733 for the NelderMeadSweeper, which was using another component that doesn't appear to be in the `dotnet/machinelearning` repo.\r\n\r\nWorking towards #585","Url":"https://github.com/dotnet/machinelearning/pull/734","RelatedDescription":"Open PR \"Convert ML.Sweeper usages of SubComponent to IComponentFactory.\" (#734)"}],"ResultType":"GitHubIssue"}},"RunOn":"2018-08-29T05:30:35.082096Z","RunDurationInMilliseconds":1143}