{"Data":{"GitHub":{"Issues":[{"Id":"409126387","IsPullRequest":false,"CreatedAt":"2019-02-12T05:24:35","Actor":"rogancarr","Number":"2512","RawContent":null,"Title":"SaveAsText documentation doesn't address scope of separator argument","State":"open","Body":"`SaveAsText` in the `Data` catalog has a `separator` parameter, which is specified as a `char`. However, this throws an `ArgumentOutOfRangeException` if the `separator` is not a \"space, tab, comma, semicolon, or bar\".\r\n\r\nCurrently, the only way to discover this delimiter limitation is to try and fail.\r\n\r\nWe have a few options. We could allow any character to be used as a delimiter, we could add documentation to the parameter that says it must be one of these characters, or, if we really do want to scope to a limited set of characters, then we could add a compile-time check by changing this to an `enum`.","Url":"https://github.com/dotnet/machinelearning/issues/2512","RelatedDescription":"Open issue \"SaveAsText documentation doesn't address scope of separator argument\" (#2512)"},{"Id":"408363212","IsPullRequest":false,"CreatedAt":"2019-02-12T04:00:18","Actor":"nsulikowski","Number":"2484","RawContent":null,"Title":"How about a Preview for Metadata?","State":"closed","Body":"something like this?\r\n`DataDebuggerPreview Preview(this Metadata metadata, int maxRows = 100)`","Url":"https://github.com/dotnet/machinelearning/issues/2484","RelatedDescription":"Closed issue \"How about a Preview for Metadata?\" (#2484)"},{"Id":"409103426","IsPullRequest":true,"CreatedAt":"2019-02-12T03:28:09","Actor":"codemzs","Number":"2511","RawContent":null,"Title":"Lockdown Microsoft.ML.FastTree public surface","State":"open","Body":"fixes #2266 \r\n\r\n**Reduces public API count from 1098 to 274.**\r\n\r\n| Before | After |   \r\n|:-:|:-:|\r\n| ![image](https://user-images.githubusercontent.com/1211949/52609628-bddb0200-2e32-11e9-9687-212f6d2db92c.png) | ![image](https://user-images.githubusercontent.com/1211949/52609436-29709f80-2e32-11e9-96ea-c5deb61782db.png) ||\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/2511","RelatedDescription":"Open PR \"Lockdown Microsoft.ML.FastTree public surface\" (#2511)"},{"Id":"409071847","IsPullRequest":true,"CreatedAt":"2019-02-12T01:44:01","Actor":"artidoro","Number":"2510","RawContent":null,"Title":"Creation of components through MLContext, internalization, and renaming","State":"open","Body":"Fixes #1798, #1758 \r\n\r\n1. In this PR I internalize the constructors of `DnnImageFeaturizerEstimator`, `PriorTrainer`, and `RandomTrainer`. This required to add catalog extensions for these three trainers. Internalizing these constructor closes issue #1798. \r\n2. I also rename a few instances of `Arguments` to `Options` which closes issue #1758.\r\n3. I internalized some fields across `ITransformers` and `IEstimators`.\r\n4. I also moved the `Options` class in TensorflowTransform to the estimator. ","Url":"https://github.com/dotnet/machinelearning/pull/2510","RelatedDescription":"Open PR \"Creation of components through MLContext, internalization, and renaming\" (#2510)"},{"Id":"409070226","IsPullRequest":true,"CreatedAt":"2019-02-12T01:36:40","Actor":"Ivanidzo4ka","Number":"2509","RawContent":null,"Title":"Internalize IDataTransform","State":"open","Body":"Hide the unhideable, bestfriend the unfrienable!\r\nfixes https://github.com/dotnet/machinelearning/issues/1995","Url":"https://github.com/dotnet/machinelearning/pull/2509","RelatedDescription":"Open PR \"Internalize IDataTransform\" (#2509)"},{"Id":"409018745","IsPullRequest":false,"CreatedAt":"2019-02-11T23:07:43","Actor":"rogancarr","Number":"2508","RawContent":null,"Title":"Create functional tests for all V1 Data Sources scenarios","State":"open","Body":"As laid out in #2498 , we need scenarios to cover the Data Sources functionality we want fully supported in V1.\r\n\r\nWe need to be able to read from\r\n- IEnumerable\r\n- Delimited files","Url":"https://github.com/dotnet/machinelearning/issues/2508","RelatedDescription":"Open issue \"Create functional tests for all V1 Data Sources scenarios\" (#2508)"},{"Id":"408971935","IsPullRequest":false,"CreatedAt":"2019-02-11T22:29:47","Actor":"rogancarr","Number":"2502","RawContent":null,"Title":"Validation training isn't supported by the new API","State":"closed","Body":"I cannot find a way to train with a validation set in the new API, and I think there is some confusion as to how this would work, were it possible.\r\n\r\nTake this example:\r\n\r\n```cs\r\n// Create a pipeline to train on the sentiment data\r\nvar trainData = readTrainData();\r\nvar validData = readValidData();\r\n\r\nvar pipeline = mlContext.Transforms.SomTransform.()\r\n    .AppendCacheCheckpoint(mlContext) as IEstimator<ITransformer>;\r\nvar preprocessor = pipeline.Fit(trainData);\r\nvar preprocessedValidData = preprocessor.Transform(validData);\r\n\r\n// Train model with validation set.\r\n// There is no way below to specify a validation set for the learner\r\npipeline = pipeline.Append(mlContext.Regression.Trainers.FastTree(numTrees: 2));\r\n// Nor is there a way to specify a validation set in the Fit\r\nvar model = pipeline.Fit(trainData);\r\n```\r\n\r\nThis example uses `FastTree`, but the same problem exists for `GAM` and `FFM`, our other validation-set trainers: There is nowhere to specify a validation set.\r\n\r\nA second problem exists, namely: If we were to specify a validaiton set, would we specify the raw, unprocessed validation set (here `validData`) or would we specify the pre-transformed validation set (here, `preprocessedValidData`).","Url":"https://github.com/dotnet/machinelearning/issues/2502","RelatedDescription":"Closed issue \"Validation training isn't supported by the new API\" (#2502)"},{"Id":"408998651","IsPullRequest":true,"CreatedAt":"2019-02-11T22:06:34","Actor":"Ivanidzo4ka","Number":"2507","RawContent":null,"Title":"Get rid of value tuples in TrainTest and CrossValidation","State":"open","Body":"Fixes #2501","Url":"https://github.com/dotnet/machinelearning/pull/2507","RelatedDescription":"Open PR \"Get rid of value tuples in TrainTest and CrossValidation\" (#2507)"},{"Id":"408996817","IsPullRequest":true,"CreatedAt":"2019-02-11T22:01:24","Actor":"wschin","Number":"2506","RawContent":null,"Title":"[WIP] Typed SDCA binary trainers","State":"open","Body":"Make `SdcaBinaryTrainer` strongly-typed according to the type it produces. The existing `SdcaBinaryTrainer` can produce either calibrated linear model or linear model as mentioned in #2469. To fix #2469, we move common functionalities used in both cases to `SdcaBinaryTrainerBase<T>` where `T` is `CalibratedModelParametersBase<LinearBinaryModelParameters, PlattCalibrator` for calibrated case and `LinearBinaryModelParameters` otherwise. Top-level APIs are changed accordingly. For dynamic/static APIs, we have 4 SDCA binary trainers for\r\n\r\n1. Calibrated linear model with simple arguments.\r\n2. Calibrated linear model with advanced options.\r\n3. Uncalibrated linear model with simple arguments.\r\n4. Uncalibrated linear model with advanced options.\r\n\r\nIn addition, because we don't like auto-calibration, the output schema in uncalibrated cases should not contain a probability column. This PR also fixes this in the two derived trainer classes' `ComputeSdcaBinaryClassifierSchemaShape`; the only case with a probability column generated is training logistic regression with Sdca.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/2506","RelatedDescription":"Open PR \"[WIP] Typed SDCA binary trainers\" (#2506)"},{"Id":"408990471","IsPullRequest":true,"CreatedAt":"2019-02-11T21:44:04","Actor":"yaeldekel","Number":"2505","RawContent":null,"Title":"Move IModelCombiner to Microsoft.ML.Data","State":"open","Body":"We have an implementation of IModelCombiner in Microsoft.ML.FastTree. This PR is to move the interface to Microsoft.ML.Data so that Microsoft.ML.FastTree doesn't need to reference Microsoft.ML.Ensemble.","Url":"https://github.com/dotnet/machinelearning/pull/2505","RelatedDescription":"Open PR \"Move IModelCombiner to Microsoft.ML.Data\" (#2505)"},{"Id":"408987057","IsPullRequest":true,"CreatedAt":"2019-02-11T21:35:07","Actor":"sfilipi","Number":"2504","RawContent":null,"Title":"[WIP] towards 1529: replacing the predicates with an IEnumerable on IRowToRowMapper.GetDependencies","State":"open","Body":"More work towards #1529. \r\n\r\nMarked the pr as still working on it, because there is one test failing: TestAndPredictoOnIris; double-checking the changes on the CompositeRowToRowMapper. \r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/2504","RelatedDescription":"Open PR \"[WIP] towards 1529: replacing the predicates with an IEnumerable on IRowToRowMapper.GetDependencies\" (#2504)"},{"Id":"408972600","IsPullRequest":true,"CreatedAt":"2019-02-11T20:57:36","Actor":"rogancarr","Number":"2503","RawContent":null,"Title":"Add validation scenario tests","State":"open","Body":"This PR adds a test for training with a validation set.\r\n\r\nFixes #2499","Url":"https://github.com/dotnet/machinelearning/pull/2503","RelatedDescription":"Open PR \"Add validation scenario tests\" (#2503)"},{"Id":"408949370","IsPullRequest":false,"CreatedAt":"2019-02-11T19:55:12","Actor":"rogancarr","Number":"2501","RawContent":null,"Title":"Remove value-tuples from the public surface of the API","State":"open","Body":"The public surface of the API contains value-tuples, which present a few problems:\r\n- We can't change the data we return in future releases (e.g. add functionality)\r\n- They don't play well with F#\r\n- Support in VS isn't great\r\n\r\nThese occur are in at least\r\n- `CrossValidation` (e.g. for each task)\r\n- `TrainTestSplit`: (e.g. for each task)\r\n\r\nWe should make sure that no value-tuples are returned and that they are not in the parameters of any public API.\r\n\r\nRelated to: #2487 ","Url":"https://github.com/dotnet/machinelearning/issues/2501","RelatedDescription":"Open issue \"Remove value-tuples from the public surface of the API\" (#2501)"},{"Id":"408921384","IsPullRequest":false,"CreatedAt":"2019-02-11T19:53:21","Actor":"fwaris","Number":"2496","RawContent":null,"Title":"\"UnbalancedSets\" not available on FastTree in v0.10","State":"closed","Body":"Somehow the ability to set UnbalancedSets flag on FastTree (at least) is gone from v 0.10 API.\r\n\r\nIt was available (via a delegate) in v0.90.","Url":"https://github.com/dotnet/machinelearning/issues/2496","RelatedDescription":"Closed issue \"\"UnbalancedSets\" not available on FastTree in v0.10\" (#2496)"},{"Id":"408947750","IsPullRequest":false,"CreatedAt":"2019-02-11T19:50:58","Actor":"jignparm","Number":"2500","RawContent":null,"Title":"Enable OnnxTransform tests on Ubuntu, Disable on CentOS","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: Ubuntu 16.04\r\n- **.NET Version (eg., dotnet --info)**: 2.1\r\n\r\n### Issue\r\n\r\nOnnxTransform tests need to be activated on  new Ubuntu leg (when ready)\r\n\r\nOnnxTransform tests need to be deactivated on  CentOS leg (glibc version is not compatible).\r\n\r\nUpdate OnnxTransform to throw friendly error  message when running on incompatible Linux distro.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2500","RelatedDescription":"Open issue \"Enable OnnxTransform tests on Ubuntu, Disable on CentOS\" (#2500)"},{"Id":"408946115","IsPullRequest":false,"CreatedAt":"2019-02-11T19:46:48","Actor":"rogancarr","Number":"2499","RawContent":null,"Title":"Create functional tests for all V1 validation scenarios","State":"open","Body":"As laid out in #2498 , we need scenarios to cover the Validation functionality we want fully supported in V1.\r\n\r\n- Cross-validation (already covered)\r\n- Validation","Url":"https://github.com/dotnet/machinelearning/issues/2499","RelatedDescription":"Open issue \"Create functional tests for all V1 validation scenarios\" (#2499)"},{"Id":"408941928","IsPullRequest":false,"CreatedAt":"2019-02-11T19:35:40","Actor":"rogancarr","Number":"2498","RawContent":null,"Title":"V1 Scenarios need to be covered by tests","State":"open","Body":"In issue #584, we laid out a set of scenarios that we'd like to cover for V1.0 of ML.NET. We need high-level functional tests to make sure that these work well in the 1.0 library.\r\n\r\nHere is a list of tests that cover the scenarios. Let's use this issue as a top-level issue to track coverage of the APIs.\r\n\r\nCategory | Scenarios | Link to Test | Completed PR | Blocked by Issue\r\n-- | -- | -- | -- | --\r\nData   Sources | I can   use objects already in memory (as IEnumerable) as input to my ML   pipeline/experiment |   |   |  \r\nData   Sources | I can   use locally stored delimited files (.csv, .tsv, etc.) as input to my ML   pipeline/experiment |   |   |  \r\n~~Data   Sources~~ | ~~I can   use data from a SQL database by reading it into memory or to disk using an   existing SQL reader and then use that as input to my ML pipeline/experiment~~ (Covered by delimited and enumerable use cases.) |   |   |  \r\nData   Transformation, Feature Engineering | I can   take an existing ONNX model and get predictions from it (as both final output   and as input to downstream pipelines) |   |   |  \r\nData   Transformation, Feature Engineering | Extensible   transformation: It should be possible to write simple row-mapping transforms.       Examples:   \"I can add custom steps to my pipeline such as creating a new column   that is the addition of two other columns, or easily add cosine similarity,   without having to create my own build of ML.NET.\" |   |   |  \r\nData   Transformation, Feature Engineering | I can   modify settings in the TextFeaturizer to update the number of word-grams and   char-grams used along with things like the normalization. |   |   |  \r\nData   Transformation, Feature Engineering | I can   apply normalization to the columns of my data |   |   |  \r\nData   Transformation, Feature Engineering | I can   take an existing TF model and get predictions from it or any layer in the   model |   |   |  \r\nData   Transformation and Feature Engineering | P1: I   can take an existing TF model and use ML.NET APIs to identify the input and   output nodes |   |   |  \r\nDebugging | I can see how each transform affects my data. An example   from Tom: E.g.: if I were to have the text \"Help I'm a   bug!\" I   should be able to see the steps where it is normalized to \"help i'm a bug\" then tokenized into [\"help\",   \"i'm\", \"a\", \"bug\"] then mapped into term   numbers [203,   25, 3, 511] then   projected into the sparse float vector {3:1, 25:1, 203:1, 511:1}, etc. etc. |   |   |  \r\nDebugging | I can   see how my data was read in to verify that I specified the schema correctly |   |   |  \r\nDebugging | I can   see the output at the end of my pipeline to see which columns are available   (score, probability, predicted label) |   |   |  \r\nDebugging | I can   look at intermediate steps of the pipeline to debug my model.       Example:   > I   were to have the text `\"Help I'm a bug!\"` I should be able to see   the steps where it is normalized to `\"help i'm a bug\"` then   tokenized into `[\"help\", \"i'm\", \"a\",   \"bug\"]` then mapped into term numbers `[203, 25, 3, 511]` then   projected into the sparse float vector `{3:1, 25:1, 203:1, 511:1}`, etc. etc. |   |   |  \r\nDebugging | P1: I   can access the information needed for understanding the progress of my   training (e.g. number of trees trained so far out of how many) |   |   |  \r\nEvaluation | I can   get predictions (scores, probabilities, predicted labels) for every row in a   test dataset |   |   |  \r\nEvaluation | I can   evaluate a model trained for any of my tasks on test data. The evaluation   outputs metrics that are relevant to the task (e.g. AUC, accuracy, P/R, and   F1 for binary classification) |   |   |  \r\nEvaluation | I can   reconfigure the threshold of my binary classification model based on analysis   of the PR curves or other metrics scores. | [Link](https://github.com/dotnet/machinelearning/blob/master/test/Microsoft.ML.Functional.Tests/Prediction.cs) |   |  #2465\r\nEvaluation | (Might   not work?)   I can   map the score/probability for each class to the original class labels I   provided in the pipeline (multiclass, binary classification). |   |   |  \r\nEvaluation | P1: I   can get the data that will allow me to plot PR curves |   |   |  \r\nExplainability   & Interpretability | I can   get near-free (local) feature importance for scored examples (Feature   Contributions) |   |   |  \r\nExplainability   & Interpretability | I can   view how much each feature contributed to each prediction for trees and   linear models (Feature Contributions) |   |   |  \r\nExplainability   & Interpretability | I can   view the overall importance of each feature (Permutation Feature Importance,   GetFeatureWeights) |   |   |  \r\nExplainability   & Interpretability | I can   train interpretable models (linear model, GAM) |   |   |  \r\nIntrospective   training | I can   take an existing model file and inspect what transformers were included in   the pipeline |   |   |  \r\nIntrospective   training | I can   inspect the coefficients (weights and bias) of a linear model without much   work. Easy to find via auto-complete. |   |   |  \r\nIntrospective   training | I can   inspect the normalization coefficients of a normalizer in my pipeline without   much work. Easy to find via auto-complete. |   |   |  \r\nIntrospective   training | I can   inspect the trees of a boosted decision tree model without much work. Easy to   find via auto-complete. |   |   |  \r\nIntrospective   training | I can   inspect the topics after training an LDA transform. Easy to find via   auto-complete. |   |   |  \r\nIntrospective   training | I can   inspect a categorical transform and see which feature values map to which key   values. Easy to find via auto-complete. |   |   |  \r\nIntrospective   training | P1: I   can access the GAM feature histograms through APIs |   |   |  \r\nModel   files | I can   train a model and save it as a file. This model includes the learner as well   as the transforms   (e.g.   Decomposability) |   |   |  \r\nModel   files | I can   use a model file in a completely different process to make predictions.    (e.g.   Decomposability) |   |   |  \r\nModel   files | I can   use newer versions of ML.NET with ML.NET model files of previous versions   (for v1.x) |   |   |  \r\nModel   files | I can   easily figure out which NuGets (and versions) I need to score an ML.NET model |   |   |  \r\nModel   files | P2: I   can move data between NimbusML and ML.NET (using IDV). Prepare with NimbusML   and load with ML.NET |   |   |  \r\nModel   files | P2: I   can use model files interchangeably between compatible versions of ML.NET and   NimbusML. |   |   |  \r\nSaving   Data | I can   go through any arbitrary data transformation / model training and save the   output to disk as a delimited file (.csv, .tsv, etc.). |   |   |  \r\nSaving   Data | I can   go through any arbitrary data transformation / model training and convert the   output to an IEnumerable. |   |   |  \r\nSaving   data | P1: I   can export ML.NET models to ONNX (limited to the existing internal   functionality) |   |   |  \r\nSaving   Data | I can   save a model to text |   |   |  \r\nTasks | I can   train a model to do classification (binary and multiclass) |   |   |  \r\nTasks | I can   train a model to do regression |   |   |  \r\nTasks | I can   train a model to do anomaly detection |   |   |  \r\nTasks | I can   train a model to do recommendations |   |   |  \r\nTasks | I can   train a model to do ranking |   |   |  \r\nTasks | I can   train a model to do clustering |   |   |  \r\nTraining | I can   provide multiple learners and easily compare evaluation metrics between them. |   |   |  \r\nTraining | I can   use an initial predictor to update/train the model for some trainers (e.g.   linear learners like averaged perceptron). Specifically, start the weights   for the model from the existing weights. |   |   |  \r\nTraining | Metacomponents   smartly restrict their use to compatible components.       Example:   \"When specifying what trainer OVA should use, a user will be able to   specify any binary classifier. If they specify a regression or multi-class   classifier ideally that should be a compile error.\" |   |   |  \r\nTraining | I can   train TF models when I bring a TF model topology |   |   |  \r\nTraining | I can   use OVA and easily add any binary classifier to it |   |   |  \r\nUse in   web environments | I can   use ML.NET models to make predictions in multi-threaded environments like   ASP.NET. (This doesn't have to be inherent in the prediction engine but   should be easy to do.) |   |   |  \r\nValidation | Cross-validation:   I can take a pipeline and easily do cross validation on it without having to   know how CV works.       Have a   mechanism to do cross validation, that is, you come up with a data source   (optionally with stratification column), come up with an instantiable   transform and trainer pipeline, and it will handle (1) splitting up the data,   (2) training the separate pipelines on in-fold data, (3) scoring on the   out-fold data, (4) returning the set of evaluations and optionally trained   pipes. (People always want metrics out of xfold, they _sometimes_ want the   actual models too.) | [Link](https://github.com/dotnet/machinelearning/blob/master/test/Microsoft.ML.Functional.Tests/Validation.cs)  |  #2470 |  \r\nValidation | I can   use a validation set in a pipeline for learners that support them (e.g.   FastTree, GAM) |   | #2503 (*in progress*) |  \r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2498","RelatedDescription":"Open issue \"V1 Scenarios need to be covered by tests\" (#2498)"},{"Id":"408931468","IsPullRequest":true,"CreatedAt":"2019-02-11T19:09:01","Actor":"Ivanidzo4ka","Number":"2497","RawContent":null,"Title":"Lockdown HAL Project","State":"open","Body":"Fixes #https://github.com/dotnet/machinelearning/issues/2267","Url":"https://github.com/dotnet/machinelearning/pull/2497","RelatedDescription":"Open PR \"Lockdown HAL Project\" (#2497)"},{"Id":"408905116","IsPullRequest":false,"CreatedAt":"2019-02-11T17:58:46","Actor":"CESARDELATORRE","Number":"2495","RawContent":null,"Title":"Create sample/test on ImageLoadingEstimator for TensorFlow scoring for loading an in-memory image stream (instead of files from the drive)","State":"open","Body":"Related to [this issue](https://github.com/dotnet/machinelearning/issues/2121), we need a clear/simple sample code/test for ImageLoadingEstimator for TensorFlow scoring so it loads an in-memory image stream as input (instead image files from a drive).\r\n\r\nI couldn't see a clear way to do it here (Mentioned by Gleb): https://github.com/dotnet/machinelearning/blob/master/test/Microsoft.ML.Tests/ImagesTests.cs\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2495","RelatedDescription":"Open issue \"Create sample/test on ImageLoadingEstimator for TensorFlow scoring for loading an in-memory image stream (instead of files from the drive)\" (#2495)"},{"Id":"408414358","IsPullRequest":false,"CreatedAt":"2019-02-11T17:20:31","Actor":"sdg002","Number":"2485","RawContent":null,"Title":"CookBook sample - Could not find input column 'CategoricalOneHot'","State":"closed","Body":"### System information\r\n\r\n- **OS version/distro**:Windows 10\r\n- **.NET Version (eg., dotnet --info)**:  4.6\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\nI am trying to follow the sample code from the GitHub article\r\nhttps://github.com/dotnet/machinelearning/blob/master/docs/code/MlNetCookBook.md#how-do-i-train-my-model-on-categorical-data\r\nand get an understanding of how to get one-hot encoding to work\r\n- **What happened?**\r\nRun time error message - Could not find input column 'CategoricalOneHot'\r\nParameter name: inputSchema'\r\n\r\n- **What did you expect?**\r\nI expected the code to run without errors and then I should be able to examine how the data has been transformed\r\n\r\n### Source code / logs\r\n            // Build several alternative featurization pipelines.\r\n            var pipeline =\r\n                // Convert each categorical feature into one-hot encoding independently.\r\n                mlContext.Transforms.Categorical.OneHotEncoding(\"CategoricalFeatures\", \"CategoricalOneHot\")\r\n                // Convert all categorical features into indices, and build a 'word bag' of these.\r\n                .Append(mlContext.Transforms.Categorical.OneHotEncoding(\"CategoricalFeatures\", \"CategoricalBag\", Microsoft.ML.Transforms.Categorical.OneHotEncodingTransformer.OutputKind.Bag))\r\n                // One-hot encode the workclass column, then drop all the categories that have fewer than 10 instances in the train set.\r\n                .Append(mlContext.Transforms.Categorical.OneHotEncoding(\"Workclass\", \"WorkclassOneHot\"))\r\n                .Append(mlContext.Transforms.FeatureSelection.SelectFeaturesBasedOnCount(\"WorkclassOneHot\", \"WorkclassOneHotTrimmed\", count: 10));\r\n\r\nPlease paste or attach the code or logs or traces that would be helpful to diagnose the issue you are reporting.\r\n![image](https://user-images.githubusercontent.com/20245330/52518944-8c541200-2c4b-11e9-8b9c-0f0f12fb3b20.png)\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2485","RelatedDescription":"Closed issue \"CookBook sample - Could not find input column 'CategoricalOneHot'\" (#2485)"},{"Id":"408724660","IsPullRequest":true,"CreatedAt":"2019-02-11T17:19:55","Actor":"jwood803","Number":"2494","RawContent":null,"Title":"Update cookbook","State":"closed","Body":"Update a Cookbook section to switch the parameters for OneHotEncoding for issue #2485.","Url":"https://github.com/dotnet/machinelearning/pull/2494","RelatedDescription":"Closed or merged PR \"Update cookbook\" (#2494)"},{"Id":"408550929","IsPullRequest":false,"CreatedAt":"2019-02-11T11:55:25","Actor":"RobinSmits","Number":"2490","RawContent":null,"Title":"How to use Options.TreeBooster.Arguments","State":"closed","Body":"I currently have an ML.Net project based on version 0.10.0. As a starter into ML.Net  I'am trying to remake one of my python ML projects into ML.Net.\r\n\r\nI'am running on Windows 10 with .NET Core 3.0.100-preview-010184.\r\n\r\nI have a pipeline setup (and working) in the following way:\r\n\r\n`var pipeline = mlContext.Transforms.Concatenate(\"Features\", \"Feat1\", \"Feat2\", \"Feat3\")\r\n                                                .Append(mlContext.BinaryClassification.Trainers.LightGbm(numLeaves: 200,\r\n                                                                                                        numBoostRound: 1000,\r\n                                                                                                        minDataPerLeaf: 200,\r\n                                                                                                        learningRate: 0.05,\r\n                                                                                                        labelColumn: \"Label\"));`\r\n\r\nI would like to further specify the LightGBM parameters as are available in Options.TreeBooster.Arguments (specifically FeatureFraction and MaxDepth) but I can't figure out how it should be added or appended to the pipeline. Also I have not been able to find any samples or documentation.\r\n\r\nIs it possible to give me some information or clarification about this?","Url":"https://github.com/dotnet/machinelearning/issues/2490","RelatedDescription":"Closed issue \"How to use Options.TreeBooster.Arguments\" (#2490)"},{"Id":"408583803","IsPullRequest":true,"CreatedAt":"2019-02-11T10:52:28","Actor":"jwood803","Number":"2493","RawContent":null,"Title":"Add Light GBM sample","State":"closed","Body":"Add Light GBM sample as part of #1209.","Url":"https://github.com/dotnet/machinelearning/pull/2493","RelatedDescription":"Closed or merged PR \"Add Light GBM sample\" (#2493)"},{"Id":"408581650","IsPullRequest":false,"CreatedAt":"2019-02-10T21:25:15","Actor":"sob3kx","Number":"2492","RawContent":null,"Title":"Is custom preprocessing available?","State":"open","Body":"Hi,\r\n\r\nI am working on productivization of Keras's VGG16 model in C#. The VGG16's weights are adapted from Caffe and demands turning image into BGR and substracting mean of channels over dataset from the channels of pixel. If my explaination is not understandable, you can see the code [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py) in the first else branch of _preprocess_numpy_input. \r\n\r\nIs there an option to do such thing in ML.NET? If not, is there an option to skip the graph definition of preprocessing steps and defining own preprocessing? If not, are there any plans for such feature?\r\n\r\nThanks","Url":"https://github.com/dotnet/machinelearning/issues/2492","RelatedDescription":"Open issue \"Is custom preprocessing available?\" (#2492)"},{"Id":"408562362","IsPullRequest":true,"CreatedAt":"2019-02-10T18:16:31","Actor":"sfilipi","Number":"2491","RawContent":null,"Title":"Towards 2326: Microsoft.ML.Ensemble and Microsoft.ML.TimeSeries namespace rationalization","State":"open","Body":"Organizing the Microsoft.ML.Ensemble and Microsoft.ML.TimeSeries as described in #2326 ","Url":"https://github.com/dotnet/machinelearning/pull/2491","RelatedDescription":"Open PR \"Towards 2326: Microsoft.ML.Ensemble and Microsoft.ML.TimeSeries namespace rationalization\" (#2491)"},{"Id":"408527140","IsPullRequest":false,"CreatedAt":"2019-02-10T11:48:46","Actor":"jwood803","Number":"2489","RawContent":null,"Title":"Exception on ReadFromEnumerable with NULL values data","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**:\r\n OS Name:     Windows\r\n OS Version:  10.0.17134\r\n OS Platform: Windows\r\n RID:         win10-x64\r\n- **.NET Version (eg., dotnet --info)**: \r\n.NET Core SDK (reflecting any global.json):\r\n Version:   3.0.100-preview-009812\r\n\r\n### Issue\r\n- **What did you do?**\r\nLoaded the [wine data](https://www.kaggle.com/rajyellow46/wine-quality) into a database and read data to use in `ReadFromEnumerable`\r\n\r\n- **What happened?**\r\nGot the below exception when loading the data into `ReadFromEnumerable`. I believe this is because there are null values within the data.\r\n\t```\r\n\tSystem.ArgumentOutOfRangeException: 'Could not determine an IDataView type for member \r\n\tParameter name: rawType'\r\n\t```\r\n- **What did you expect?**\r\n\r\n### Source code / logs\r\nHave this ML.NET code:\r\n```\r\nvar dbData = ReadFromDatabase();\r\n\r\nvar context = new MLContext();\r\n\r\nvar mlData = context.Data.ReadFromEnumerable(dbData);\r\n```\r\n\r\nAnd a preview of `dbData` where I think it's failing at:\r\n```\r\n[\r\n  {\r\n    \"Type\": \"white\",\r\n    \"FixedAcidity\": null,\r\n    \"VolatileAcidity\": 1.0,\r\n    \"CitricAcid\": 0.0,\r\n    \"ResidualSugar\": 1.0,\r\n    \"Chlorides\": 0.0,\r\n    \"FreeSulfurDioxide\": 29.0,\r\n    \"TotalSulfureDioxide\": 75.0,\r\n    \"Density\": 1.0,\r\n    \"Ph\": 3.0,\r\n    \"Sulphates\": 0.0,\r\n    \"Alcohol\": 13.0,\r\n    \"Quality\": 8.0\r\n  }\r\n]\r\n```\r\n\r\nAlso, is this the correct way to handle missing data when reading from a database instead of from a CSV file? Or should I be handling it another way?","Url":"https://github.com/dotnet/machinelearning/issues/2489","RelatedDescription":"Open issue \"Exception on ReadFromEnumerable with NULL values data\" (#2489)"},{"Id":"408519971","IsPullRequest":false,"CreatedAt":"2019-02-10T10:08:19","Actor":"Ikaer","Number":"2488","RawContent":null,"Title":"ProductRecommander sample - System.AccessViolationException","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: Win10\r\n- **.NET Version (eg., dotnet --info)**: .Net Core 2.1\r\n\r\n### Issue\r\n\r\n- **What did you do?** I tried the sample Product recommander with my own sample of product IDs. I first try it with the provided data (Amazon0302) and it works fine. Then I try a different file (which is attached).\r\n- **What happened?** An exception is throw at ITransformer model = est.Fit(traindata); System.AccessViolationException: 'Attempted to read or write protected memory. This is often an indication that other memory is corrupt.'\r\n- **What did you expect?** To not have this kind of exception at least (this is scary) and to be able to use a different sample than the one provided.\r\nI have the same error if, for example, I truncate the file Amazon0302 to only keep the first 10 lines.\r\n\r\n\r\n### Source code / logs\r\n[test_sample.txt](https://github.com/dotnet/machinelearning/files/2848725/test_sample.txt)\r\nI just downloaded the ProductRecommander sample and replace the variable TrainingDataLocation with my own file. I reproduced the problem on two machines (one with Win10, another one with Windows Server 2016, both with .net core 2.1)\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2488","RelatedDescription":"Open issue \"ProductRecommander sample - System.AccessViolationException\" (#2488)"},{"Id":"408485549","IsPullRequest":false,"CreatedAt":"2019-02-10T00:19:01","Actor":"rogancarr","Number":"2487","RawContent":null,"Title":"Cross-Validation API for v1.0","State":"open","Body":"Hi All,\r\n\r\nAs we approach v1.0, I thought it might be nice to look at the API for cross-validation. Currently, our cross-validation API takes the inputs:\r\n\r\n```cs\r\nIDataView data; // training data\r\nIEstimator<ITransformer> estimator; // Model to fit\r\nint numFolds; //Number of folds to make\r\nstring labelColumn; // The label\r\nstring stratificationColumn; // The column to stratify on\r\nseed; // The seed\r\n```\r\n\r\nand returns an array of\r\n```cs\r\nRegressionMetrics metrics;\r\nITransformer model;\r\nIDataView scoredTestData;\r\n```\r\nwith one entry for each fold.\r\n\r\nI have a few questions:\r\n\r\n1) Are we happy with the outputs?\r\nI'm not overly concerned with these, but it will be hard to make this list smaller as we go.\r\n2) Do we need to specify `labelColumn`?\r\nIsn't there a way to get the label from the model? Making this explicit means that we are allowing the learner and the CV metrics to utilize different labels.\r\n3) Are we using the right terminology for `stratification`?\r\nStratification usually means that ratios of classes are maintained across splits (see [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) on wikipedia). Here, `stratification` means that items with the same value are clumped into the same split. The former makes sense if you want to maintain class ratios, especially with highly imbalanced classes, while the latter is useful for things like ranking (e.g. `groupIds`) or where leakage due to something like ordering may be a concern.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2487","RelatedDescription":"Open issue \"Cross-Validation API for v1.0\" (#2487)"},{"Id":"408418151","IsPullRequest":false,"CreatedAt":"2019-02-09T10:16:39","Actor":"DevLob-zz","Number":"2486","RawContent":null,"Title":"StochasticDualCoordinateAscent not work For Multiclass after migrate to 0.10.0","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: W10\r\n- **.NET Version (eg., dotnet --info)**:  4.6.1\r\n\r\n### Issue\r\n\r\n- **What did you do?** Migrated my code from 0.9.0 to 0.10.0\r\n- **What happened?** StochasticDualCoordinateAscent Algorithm was working fine  for multi class and binary trainer  before move to 0.10 after updating its working only for binary and freeze on \r\ntrainingPipeline.Fit(trainingDataView); for Multiclass take a while \r\n- **What did you expect?**\r\nit should work fine \r\n### Source code / logs\r\n`var mlContext = new MLContext(seed: 1);\r\n\r\n#region \"STEP 1: Common data loading configuration\"\r\n\r\n    IDataView trainingDataView = GetNormalDataSet(mlContext, allFeatures, mLFeatures);\r\n\r\n    if (trainingDataView.GetRowCount() == 0)\r\n    {\r\n\r\n        return;\r\n    }\r\n\r\n    textFeatures = GetTextFeatures(normalFeatures);\r\n\r\n    numericFeatures = GetNumericFeatures(normalFeatures).ToArray();\r\n\r\n#endregion\r\n\r\n#region \"STEP 2: Common data process configuration with pipeline data transformations\"\r\n\r\n// STEP 2: Common data process configuration with pipeline data transformations\r\n\r\nvar textFeaturesProcessPipeline = mlContext.Transforms.Text.FeaturizeText(DefaultColumnNames.Features, textFeatures);\r\n\r\nvar numericFeaturesProcessPipeline = mlContext.Transforms.Concatenate(DefaultColumnNames.Features, numericFeatures);\r\n\r\nvar dataProcessPipeline = numericFeaturesProcessPipeline.Append(textFeaturesProcessPipeline).AppendCacheCheckpoint(mlContext);\r\n\r\n#endregion\r\n\r\n#region  \"STEP 3: Set the training algorithm, then create and configure the modelBuilder\"  \r\n\r\nITransformer trainedModel = null;\r\n\r\n//\"StochasticDualCoordinateAscent\"\r\n\r\nvar trainingPipeline = mlContext.Transforms.Conversion.MapValueToKey(DefaultColumnNames.Label)\r\n   .Append(dataProcessPipeline)\r\n   .Append(mlContext.MulticlassClassification.Trainers.StochasticDualCoordinateAscent(labelColumn: DefaultColumnNames.Label, featureColumn: DefaultColumnNames.Features))\r\n   .Append(mlContext.Transforms.Conversion.MapKeyToValue(DefaultColumnNames.PredictedLabel));\r\n\r\n#region STEP 4: Train the model fitting to the DataSet\r\n\r\n    //Take a while and no responce when call fit method\r\n\r\n    trainedModel = trainingPipeline.Fit(trainingDataView);\r\n\r\n#endregion`\r\n\r\n you can see some screen for values and when change from \r\n\"StochasticDualCoordinateAscent\" to \"Naive Bayes\" Working fine\r\n\r\nwhat wrong on my code \r\n![image](https://user-images.githubusercontent.com/5037612/52519439-04c6cd00-2c64-11e9-9e11-64808c1d22cd.png)\r\n![image](https://user-images.githubusercontent.com/5037612/52519443-0b554480-2c64-11e9-9116-8d17f4ff02e1.png)\r\n![image](https://user-images.githubusercontent.com/5037612/52519455-2e7ff400-2c64-11e9-8e8c-bd0684ad988f.png)\r\n\r\nalso those my Data Structure Classes \r\n`[Serializable]\r\n    public class NormalTagsModelFeatures\r\n    {\r\n        //[Column(ordinal: \"0\", name: \"Label\")] public string Label;\r\n        [LoadColumn(0)]\r\n        public string Label;\r\n        [LoadColumn(1)]\r\n        public float fontSize;\r\n        [LoadColumn(2)]\r\n        public float isBold;\r\n        [LoadColumn(3)]\r\n        public float isItalic;\r\n        [LoadColumn(4)]\r\n        public float isUnderLine;\r\n        [LoadColumn(5)]\r\n        public float containsDot;\r\n        [LoadColumn(6)]\r\n        public float containsQuestionMark;\r\n        [LoadColumn(7)]\r\n        public string fontColor;\r\n        [LoadColumn(8)]\r\n        public float isAllCaps;\r\n        [LoadColumn(9)]\r\n        public string tagText;\r\n        [LoadColumn(10)]\r\n        public string firstWord;\r\n        [LoadColumn(11)]\r\n        public string FontName;\r\n        [LoadColumn(12)]\r\n        public float verticalText;\r\n        [LoadColumn(13)]\r\n        public float trdLeft;\r\n        [LoadColumn(14)]\r\n        public float trdRight;\r\n        [LoadColumn(15)]\r\n        public float trdTop;\r\n        [LoadColumn(16)]\r\n        public float trdBottom;\r\n        [LoadColumn(17)]\r\n        public float pageNo;\r\n\r\n    }\r\n\r\n    public class NormalTagsPrediction\r\n    {\r\n        [ColumnName(\"PredictedLabel\")]\r\n        public string PredictedLabel;\r\n\r\n        [ColumnName(\"Score\")]\r\n        public float[] Score { get; set; }\r\n\r\n    }`\r\n\r\nPlease paste or attach the code or logs or traces that would be helpful to diagnose the issue you are reporting.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2486","RelatedDescription":"Open issue \"StochasticDualCoordinateAscent not work For Multiclass after migrate to 0.10.0\" (#2486)"},{"Id":"408350055","IsPullRequest":true,"CreatedAt":"2019-02-08T22:33:26","Actor":"shmoradims","Number":"2483","RawContent":null,"Title":"Documentation for BinaryClassification.AveragedPerceptron","State":"open","Body":"Docs & sample for BinaryClassification.AveragedPerceptron. Related to #1209.\r\n","Url":"https://github.com/dotnet/machinelearning/pull/2483","RelatedDescription":"Open PR \"Documentation for BinaryClassification.AveragedPerceptron\" (#2483)"}],"ResultType":"GitHubIssue"}},"RunOn":"2019-02-12T05:30:50.0220658Z","RunDurationInMilliseconds":1188}