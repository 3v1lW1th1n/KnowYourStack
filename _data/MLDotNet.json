{"Data":{"GitHub":{"Issues":[{"Id":"499573005","IsPullRequest":true,"CreatedAt":"2019-10-02T04:23:30","Actor":"KsenijaS","Number":"4260","RawContent":null,"Title":"Tensor extensions","State":"closed","Body":"Output tensors from graph are being copied from unamanaged to managed memory and every time a new buffer is created. Performance will be better if we reuse buffer when it's possible.\r\nImprove performance by replacing ToArray method in Tensor class with extension methods: ToScalar, ToSpan and ToArray\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/4260","RelatedDescription":"Closed or merged PR \"Tensor extensions\" (#4260)"},{"Id":"501263651","IsPullRequest":false,"CreatedAt":"2019-10-02T04:08:51","Actor":"nighotatul","Number":"4275","RawContent":null,"Title":"how we draw tree using fast tree algorithm?","State":"open","Body":"@ganik -  i have prepared sample on this below link.\r\nthis is Visualize sickit-learn decision trees with d3.js\r\nhttp://bl.ocks.org/fractalytics/raw/495b63cf671b4c487bc40801366384e0/\r\n\r\nwe do not know python script but we know C#. so that please helps us to draw above tree using fasttree ml.net using score,weights,bias.\r\nif any sample is their please share with us.","Url":"https://github.com/dotnet/machinelearning/issues/4275","RelatedDescription":"Open issue \"how we draw tree using fast tree algorithm?\" (#4275)"},{"Id":"499710603","IsPullRequest":true,"CreatedAt":"2019-10-02T03:58:43","Actor":"antoniovs1029","Number":"4262","RawContent":null,"Title":"Addresses #3976 about using PFI with a model loaded from disk","State":"closed","Body":"With this pull request it is now possible to use PFI with some models loaded from disk. **This is not yet a final solution to the problem**, as described in the last section below.\r\n\r\n### The Problem\r\nAs explained in [this comment](https://github.com/dotnet/machinelearning/issues/3976#issuecomment-517862076) of issue #3976 it was not possible to use PFI with a model loaded from disk, because the last transformer of the loaded model was not of the appropriate type to use it with the `PermutationFeatureImportance `method.\r\n\r\nSpecifically the problem occurred because when loading the last transformer, a 'create' method would be called and it would assign an inappropriate type to the last transformer, making it unusable for PFI. For example, if a model had `RegressionPredictionTransformer<OlsModelParameters>` as last transformer, and it was saved to disk, later on when loading it from disk the last transformer would be of type `RegressionPredictionTransformer<IPredictorProducing<float>>.` This would have happened because the [Create method ](https://github.com/dotnet/machinelearning/blob/bb00e07b30e9626b3578ff1934b86dad0d1d1ce9/src/Microsoft.ML.Data/Scorers/PredictionTransformer.cs#L615) that is called when loading a `RegressionPredictionTransformer ` would always return a `RegressionPredictionTransformer<IPredictorProducing<float>>` regardless of the actual `TModel `that should be loaded. In this case, it would be necessary to load the last transformer as `RegressionPredictionTransformer<OlsModelParameters>` in order to use it with PFI.\r\n\r\nThis was a problem also in the BinaryClassification, MulticlassClassification and Ranking prediction transformers which implemented a similar Create method. All of these classes are used with PFI.\r\n\r\n### The approach of the solution\r\nThe main obstacle was that the appropriate type `TModel` to be used when loading the last transformer couldn't be known at compile time, only at runtime once the last transformer was being loaded.\r\n\r\nSo to solve the problem, it was necessary to load first the internal model (e.g. the `OlsModelParameters `object) in the Create method of the prediction transformer, get its type to be used as `TModel`, make a generic type at runtime for the prediction transformer using the actual `TModel`, and instantiate that type with a constructor that would receive the internal model (previously loaded) to add it to the last transformer; this constructor would then continue to load the prediction transformer.\r\n\r\n### Changes implemented\r\n\r\n- The create method of the prediction transformers (binary, multiclass, ranking and regression) was modified to solve the problem. Different overloads of constructors were created to receive the internal model once it was loaded and then continue loading the prediction transformer.\r\n\r\n- **Samples were added based on the original PFI samples**, but now using a model that is saved and loaded from disk directly in the sample. This is provided for the multiclass, ranking and regression, but NOT for the binary classification case (see the final section below).\r\n\r\n- **Test cases based on the original PFI test cases are also provided**, but now using a model that is saved and loaded from disk directly in the test case. Again, this was done for multiclass, ranking and regression, but not for binary classification.\r\n\r\n- **Changes were made to 2 tests in the `LbfgsTests.cs`** file where PFI is not used, but prediction transformers are loaded from disk. The changes regard casts that were used in the tests, but can no longer be used. For example, `as MulticlassPredictionTransformer<IPredictorProducing<VBuffer<float>>>` was replaced for `as MulticlassPredictionTransformer<MaximumEntropyModelParameters>` in one test. This is done because now the create method returns the appropriate TModel type instead of the `IPredictorProducing<>` type. Notice that it is invalid to cast from `MulticlassPredictionTransformer<MaximumEntropyModelParameters>` to `MulticlassPredictionTransformer<IPredictorProducing<VBuffer<float>>>`... so those tests fail without the changes I made to the tests. Also notice that since `IPredictorProducing<>` is internal, a regular user using a nugget wouldn't be able to do the casts that were done in those tests.\r\n\r\n### Further problems\r\nIn this pull request I provide working samples and tests for regression, multiclass and ranking. Still, I've been unable to provide them for binary classification.\r\n\r\nThe [existing sample for PFI with binary classification](https://github.com/dotnet/machinelearning/blob/2942ca4e02d354b48767a1b01017f57cdc3fe44c/docs/samples/Microsoft.ML.Samples/Dynamic/Trainers/BinaryClassification/PermutationFeatureImportance.cs#L27) uses a last transformer of type `BinaryPredictionTransformer<CalibratedModelParameterBase<LinearBinaryModelParameters,PlattCalibrator>>` which is the type necessary to use it in the PFI method. When saving and then loading that model from disk, the last transformer was of type `BinaryPredictionTransformer<IPredictorProducing<float>>`; with the changes I made to the binary transformer, it now loads a last transformer of type `BinaryPredictionTransformer<ParameterMixingCalibratorModelParameters<IPredictorProducing<float>, ICalibrator>>`, which is still not the type necessary to use PFI.\r\n\r\nThe problem is that the [Create method of ParameterMixingCalibratorModelParameters](https://github.com/dotnet/machinelearning/blob/2942ca4e02d354b48767a1b01017f57cdc3fe44c/src/Microsoft.ML.Data/Prediction/Calibrator.cs#L564) returns a `CalibratedModelParameterBase` object. This is similar to the problem that the prediction transformers had, but there's a key difference in that `ParameterMixingCalibratorModelParameters ` is internal, and its create method returns an object of its public superclass `CalibratedModelParameterBase`. This key difference has stopped me from solving the problem using the same approach used to fix the prediction transformers. Once this pull request is accepted, I will open another issue with this specific use case, explaining it in more detail.\r\n\r\nNonetheless, notice that the problem is not in the `BinaryPredictionTransformer<TModel>` but rather in its `TModel` for this specific case. Having changed the `BinaryPredictionTransformer<TModel>` actually works as expected, in that it no longer returns a fixed `<IPredictorProducing<float>>` as TModel, and so the problem is actually in the `ParameterMixingCalibratorModelParameters `class.\r\n\r\nAlso notice that this is a signal that there might be other generic classes where the Create method returns an object with fixed type parameters that aren't the ones actually being used. This might become a problem for users trying to use PFI with a model that uses one of such classes.","Url":"https://github.com/dotnet/machinelearning/pull/4262","RelatedDescription":"Closed or merged PR \"Addresses #3976 about using PFI with a model loaded from disk\" (#4262)"},{"Id":"501037711","IsPullRequest":true,"CreatedAt":"2019-10-02T01:40:45","Actor":"bpstark","Number":"4270","RawContent":null,"Title":"Modified the project to support running of TensorFlow on GPU on Windows.","State":"closed","Body":"Removed all dependencies of TensorFlow redist from the source projects,\r\nand instead added the dependency to the Sample project.\r\nCreated separate sample project for GPU examples since gpu tensorflow requires cuda,\r\nwhich may not be available on all machines, so it needs to be a separate\r\nproject.\r\nAdded documentation for setup as there is now some setup requirements to use this API.\r\n\r\nIn testing on the large flowers data set I was able to see a large improvement in speed, from taking ~720 seconds to train to taking ~156 seconds. \r\n\r\nFixes #4269\r\n\r\nAddresses part of the issue in #86 \r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/4270","RelatedDescription":"Closed or merged PR \"Modified the project to support running of TensorFlow on GPU on Windows.\" (#4270)"},{"Id":"501034241","IsPullRequest":false,"CreatedAt":"2019-10-02T01:40:45","Actor":"bpstark","Number":"4269","RawContent":null,"Title":"TensorFlow based DNN models do not support the GPU on windows.","State":"closed","Body":"### System information\r\n\r\n- **OS version/distro**: Windows\r\n\r\n### Issue\r\n\r\nCurrently the DNN TensorFlow based models do not support GPU training/inferencing. \r\n\r\nDNNs should be able to support the GPU.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/4269","RelatedDescription":"Closed issue \"TensorFlow based DNN models do not support the GPU on windows.\" (#4269)"},{"Id":"501207590","IsPullRequest":false,"CreatedAt":"2019-10-02T00:14:38","Actor":"luisquintanilla","Number":"4274","RawContent":null,"Title":"[Image Classification API] No evaluation when batchSize parameter > # of instances in dataset","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: Windows 10\r\n- **.NET Version (eg., dotnet --info)**: .NET Core 2.2\r\n- **ML.NET Version**: 1.4.0-preview\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\n\r\nTried to train an image classification model using the Image Classification API. The value set for `batchSize` parameter is 300. Meanwhile then number of data instances in the test set is 182.\r\n\r\n- **What happened?**\r\n\r\nNo evaluation takes place. 0 batches are processed. \r\n\r\n- **What did you expect?**\r\n\r\nThe model to train and for it to evaluate the number of instances provided. In this case since the number of data instances is less than the amount set for the `batchSize` parameter, it would process 1 batch instead of 0.\r\n\r\nThe model to evaluate\r\n\r\n### Source code / logs\r\n\r\nPipeline:\r\n\r\n```csharp\r\nvar trainingPipeline =\r\n                mapLabelTransform\r\n               .Append(mlContext.Model.ImageClassification(\r\n                   featuresColumnName: \"ImagePath\",\r\n                   labelColumnName: \"LabelAsKey\",\r\n                   arch: ImageClassificationEstimator.Architecture.ResnetV2101,\r\n                   epoch: 100,\r\n                   batchSize: 300,\r\n                   testOnTrainSet: false,\r\n                   metricsCallback: (metrics) => Console.WriteLine(metrics),\r\n                   validationSet: transformedTestData,\r\n                   reuseTrainSetBottleneckCachedValues: true,\r\n                   reuseValidationSetBottleneckCachedValues: true));\r\n```\r\n\r\nOutput:\r\n\r\n```text\r\nNumber of rows 182\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   0, Epoch:  93, Accuracy:        NaN\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   0, Epoch:  94, Accuracy:        NaN\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   0, Epoch:  95, Accuracy:        NaN\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   0, Epoch:  96, Accuracy:        NaN\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   0, Epoch:  97, Accuracy:        NaN\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   0, Epoch:  98, Accuracy:        NaN\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   0, Epoch:  99, Accuracy:        NaN\r\n```\r\n\r\nWhen the `batchSize` is set equal to the number of rows (in this case 182), this is the output:\r\n\r\n```text\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   1, Epoch:  95, Accuracy:          1\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   1, Epoch:  96, Accuracy:          1\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   1, Epoch:  97, Accuracy:          1\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   1, Epoch:  98, Accuracy:          1\r\nPhase: Training, Dataset used: Validation, Batch Processed Count:   1, Epoch:  99, Accuracy:          1\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/4274","RelatedDescription":"Open issue \"[Image Classification API] No evaluation when batchSize parameter > # of instances in dataset\" (#4274)"},{"Id":"501195820","IsPullRequest":true,"CreatedAt":"2019-10-01T23:28:49","Actor":"pieths","Number":"4273","RawContent":null,"Title":"Add DateTime to DateTime standard conversion.","State":"open","Body":"This fixes an error which is caused by a missing DateTime to DateTime conversion when outputting DateTime columns in NimbusML. NimbusML calls in to `RowCursorUtils.GetGetterAsCore` (indirectly) which tries to find a conversion from DateTime to DateTime and fails with the following error:\r\n\r\n```console\r\nError: *** System.InvalidOperationException: 'No standard conversion from 'DateTime' to 'DateTime'\r\n```\r\n","Url":"https://github.com/dotnet/machinelearning/pull/4273","RelatedDescription":"Open PR \"Add DateTime to DateTime standard conversion.\" (#4273)"},{"Id":"500440844","IsPullRequest":true,"CreatedAt":"2019-10-01T23:16:16","Actor":"najeeb-kazmi","Number":"4265","RawContent":null,"Title":"PFI entrypoint - Add checks for null Label, Feature, and GroupId columns","State":"closed","Body":"Related #4231 #4232 \r\n\r\nCheck whether Label, Feature, and GroupId columns are null.","Url":"https://github.com/dotnet/machinelearning/pull/4265","RelatedDescription":"Closed or merged PR \"PFI entrypoint - Add checks for null Label, Feature, and GroupId columns\" (#4265)"},{"Id":"501177421","IsPullRequest":true,"CreatedAt":"2019-10-01T22:26:35","Actor":"frank-dong-ms","Number":"4272","RawContent":null,"Title":"Issue 4120, add reasonable exception when user try to use OnnxSequenceType attribute without specify sequence type","State":"open","Body":"We are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [ ] There's a descriptive title that will make sense to other developers some time from now. \r\n- [ ] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [ ] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [ ] You have included any necessary tests in the same PR.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/4272","RelatedDescription":"Open PR \"Issue 4120, add reasonable exception when user try to use OnnxSequenceType attribute without specify sequence type\" (#4272)"},{"Id":"501176332","IsPullRequest":false,"CreatedAt":"2019-10-01T22:23:05","Actor":"sergioprates","Number":"4271","RawContent":null,"Title":"Hot to process image with 1 channel?","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**:\r\nWindows 10\r\n- **.NET Version (eg., dotnet --info)**:\r\n.NET Core 3.0 \r\n\r\n### Issue\r\nI have a model that is trained in keras that accept a input shape of 64x64x1, in this case an image in gray scale. But I don't found a way of pre-process image to send only one channel to model, in ML.NET only have a method that extract one channel, R, G, B or Alpha.\r\n- **What did you do?**\r\n`var pipeline = mlContext.Transforms.ResizeImages(resizing: ImageResizingEstimator.ResizingKind.Fill, outputColumnName: onnxModel.ModelInput, imageWidth: ImageSettings.imageWidth,\r\n                imageHeight: ImageSettings.imageHeight, inputColumnName: nameof(ImageInputData.Image))\r\n                .Append(mlContext.Transforms.ConvertToGrayscale(outputColumnName: onnxModel.ModelInput, inputColumnName: onnxModel.ModelInput))\r\n                .Append(mlContext.Transforms.ExtractPixels(outputColumnName: onnxModel.ModelInput, inputColumnName: onnxModel.ModelInput, interleavePixelColors: true, \r\n                orderOfExtraction: ImagePixelExtractingEstimator.ColorsOrder.ARGB, colorsToExtract: ImagePixelExtractingEstimator.ColorBits.Red))\r\n                .Append(mlContext.Transforms.ApplyOnnxModel(modelFile: onnxModel.ModelPath, outputColumnName: onnxModel.ModelOutput, inputColumnName: onnxModel.ModelInput));`\r\n\r\nMy image come from a input form, like onnx examples in samples repository\r\n- **What happened?**\r\nI'm getting incorrect prediction, because I don't know how to reshape image to 64x64x1, in case that I send 64x64x3 format, I'm getting the error \"Length of memory (12288) must match product of dimensions (4096).\"\r\n- **What did you expect?**\r\nI expect that ML.NET provide a way to reshape my image to one channel image to get correct predictions\r\n### Source code / logs\r\n\r\nPlease paste or attach the code or logs or traces that would be helpful to diagnose the issue you are reporting.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/4271","RelatedDescription":"Open issue \"Hot to process image with 1 channel?\" (#4271)"},{"Id":"500531241","IsPullRequest":true,"CreatedAt":"2019-10-01T00:18:26","Actor":"frank-dong-ms","Number":"4267","RawContent":null,"Title":"Fix the build badge link","State":"closed","Body":"We are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [ ] There's a descriptive title that will make sense to other developers some time from now. \r\n- [ ] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [ ] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [ ] You have included any necessary tests in the same PR.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/4267","RelatedDescription":"Closed or merged PR \"Fix the build badge link\" (#4267)"},{"Id":"500564972","IsPullRequest":false,"CreatedAt":"2019-09-30T22:46:48","Actor":"bsambrone","Number":"4268","RawContent":null,"Title":"Feature request: add \"optimize for\" in the AutoML CLI","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: Windows 10 (but applies to any)\r\n- **.NET Version (eg., dotnet --info)**: netcoreapp2.2\r\n\r\n### Issue\r\n\r\n- **What did you do?** Ran the AutoML CLI from powershell\r\n- **What happened?** It auto-trained as expected, but seemed to optimize just for accuracy. This isn't helpful when the data is highly skewed and I want to optimize for high precision or F1 score.\r\n- **What did you expect?** I was hoping to be able to specify what to optimize for just like I do when using this from C# and not the CLI.\r\n\r\nThe feature request is to add a command line flag to specify what to optimize for when running the AutoML CLI utility.\r\n","Url":"https://github.com/dotnet/machinelearning/issues/4268","RelatedDescription":"Open issue \"Feature request: add \"optimize for\" in the AutoML CLI\" (#4268)"},{"Id":"500451878","IsPullRequest":false,"CreatedAt":"2019-09-30T18:43:56","Actor":"najeeb-kazmi","Number":"4266","RawContent":null,"Title":"Evaluate PFI on raw features","State":"open","Body":"Right now, the [PFI implementation](https://github.com/dotnet/machinelearning/blob/master/src/Microsoft.ML.Transforms/PermutationFeatureImportance.cs), permutes and evaluates feature importances on the slots of the feature vector. However, I may want to evaluate PFI on the raw features (instead of or in addition to) the slots of the feature vector.\r\n\r\nConsider this scenario: PCA transform or a \"hash\" transform e.g. Text Transform with NGram Hash or Categorical Hash has been applied to raw features and the corresponding slots have no name or are unintelligible names. PFI on these slots isn't very \"explainable\" for these slots. Some sense of \"importance\" can still be had if the initial column could be permuted.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/4266","RelatedDescription":"Open issue \"Evaluate PFI on raw features\" (#4266)"},{"Id":"499922532","IsPullRequest":false,"CreatedAt":"2019-09-29T14:05:33","Actor":"nighotatul","Number":"4264","RawContent":null,"Title":"how we plot decision tree using LbfgsLogisticRegression Trainer?","State":"open","Body":"@eerhardt -  we got score and metrics but how we plot decision tree using trainer.","Url":"https://github.com/dotnet/machinelearning/issues/4264","RelatedDescription":"Open issue \"how we plot decision tree using LbfgsLogisticRegression Trainer?\" (#4264)"},{"Id":"499746780","IsPullRequest":false,"CreatedAt":"2019-09-28T09:06:28","Actor":"wpadron","Number":"4263","RawContent":null,"Title":"Duplicate column name exception when adding an Ignored column to AutoML framework","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: Windows 10.0.18362\r\n- **.NET Version (eg., dotnet --info)**: core 3.0.100\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\nAdd a column that AutoML should ignore.\r\n\r\n- **What happened?**\r\nGet an exception that the column name is duplicated.\r\n\r\n- **What did you expect?**\r\nWhen you add a column to the ignored collection you don't have to remove it from another collection (there are more than one) because the columns are inferred from the dataset at runtime.\r\n\r\n### Source code / logs\r\nColumnInformation columnInformation = columnInference.ColumnInformation;\r\ncolumnInformation.NumericColumnNames.Remove(\"payment_type\"); \r\ncolumnInformation.IgnoredColumnNames.Add(\"payment_type\");\r\n","Url":"https://github.com/dotnet/machinelearning/issues/4263","RelatedDescription":"Open issue \"Duplicate column name exception when adding an Ignored column to AutoML framework\" (#4263)"},{"Id":"499687474","IsPullRequest":true,"CreatedAt":"2019-09-28T02:51:21","Actor":"codemzs","Number":"4261","RawContent":null,"Title":"Fix code coverage yaml file.","State":"closed","Body":"Code coverage is broke due to an update that is needed in the yml file.","Url":"https://github.com/dotnet/machinelearning/pull/4261","RelatedDescription":"Closed or merged PR \"Fix code coverage yaml file.\" (#4261)"},{"Id":"499233840","IsPullRequest":true,"CreatedAt":"2019-09-27T17:10:49","Actor":"Youssef1313","Number":"4255","RawContent":null,"Title":"Typo: Retreive -> Retrieve","State":"closed","Body":"","Url":"https://github.com/dotnet/machinelearning/pull/4255","RelatedDescription":"Closed or merged PR \"Typo: Retreive -> Retrieve\" (#4255)"},{"Id":"499094871","IsPullRequest":false,"CreatedAt":"2019-09-27T16:57:55","Actor":"aslotte","Number":"4252","RawContent":null,"Title":"Jupyter + ML.NET | DataFrame vs Dataview","State":"closed","Body":"While working with ML.NET in Jupyter, it can sometimes feel like double work to have to load the data first in to a DataFrame, and then also in to a IDataView to be able to use it in e.g. a training pipeline.\r\n\r\nIt would be ideal if the `.Fit` and/or `TrainTestSplit` methods could take a DataFrame as a parameter, or if there was other interoperability methods to leverage, so that the data wouldn't need to be loaded in two places.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/4252","RelatedDescription":"Closed issue \"Jupyter + ML.NET | DataFrame vs Dataview\" (#4252)"},{"Id":"499523755","IsPullRequest":false,"CreatedAt":"2019-09-27T15:55:15","Actor":"luisquintanilla","Number":"4259","RawContent":null,"Title":"[Image Classification API] metricsCallback NullReferenceException when using custom function","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: Windows 10\r\n- **.NET Version (eg., dotnet --info)**: .NET Core 2.2\r\n- **ML.NET Version**: 1.4.0-preview\r\n\r\n### Issue\r\n\r\nTried to provide a custom function for the `metricsCallback` parameter. The documentation in IntelliSense says this method is only called during the *Training* phase. However, it does not appear to be the case. If the *Bottleneck* phase has to happen (training for first time or not using cached bottleneck values), a `System.NullReferenceException` is raised.\r\n\r\n```text\r\nSystem.NullReferenceException: 'Object reference not set to an instance of an object.'\r\n\r\ntrainMetrics was null.\r\n```\r\n\r\nI suspect this is because the same callback is used for both the Bottleneck / Training phases. It would be good to either:\r\n\r\na) Have separate callbacks depending on whether it's Bottleneck / Training phase.\r\nb) Allow the user to check which phase is currently taking place so they can display the results accordingly in their `metricsCallback` method.\r\n\r\nNote that once a model is trained and the *Bottleneck* phase no longer needs to happen since the cached values are being used, no Exceptions are raised and the application works as expected. \r\n\r\n### Source code / logs\r\n\r\nPipeline:\r\n\r\n```csharp\r\nvar trainingPipeline =\r\n    mapLabelTransform\r\n   .Append(mlContext.Model.ImageClassification(\r\n       \"ImagePath\",\r\n       \"LabelAsKey\",\r\n       arch: ImageClassificationEstimator.Architecture.ResnetV2101,\r\n       epoch: 100,\r\n       batchSize: 20,\r\n       metricsCallback: DisplayMetrics,\r\n       validationSet: transformedTestData\r\n       //reuseTrainSetBottleneckCachedValues: true,\r\n       /*reuseValidationSetBottleneckCachedValues: true*/));\r\n```\r\n\r\nMetrics Callback Method:\r\n\r\n```csharp\r\npublic static void DisplayMetrics(ImageClassificationMetrics metrics)\r\n{\r\n    TrainMetrics trainMetrics = metrics.Train;\r\n    Console.WriteLine($\"Epoch: {trainMetrics.Epoch} | Accuracy {trainMetrics.Accuracy} | Loss: {trainMetrics.CrossEntropy}\");\r\n}\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/4259","RelatedDescription":"Open issue \"[Image Classification API] metricsCallback NullReferenceException when using custom function\" (#4259)"},{"Id":"499484953","IsPullRequest":false,"CreatedAt":"2019-09-27T14:39:31","Actor":"Genysis78","Number":"4258","RawContent":null,"Title":"Model Builder reference error","State":"open","Body":"On the CLI my regression model trains without issue, and the FastTreeTweedie is automatically selected as the best performing. In the resulting files produced by the model, in the Model.cs, I am getting an error: \"The type or namespace name 'FastTreeTweedieTrainer' could not be found (are you missing a using directive or an assembly reference?) (CS0246) [VesselAnalytics]\". This seems to be linked to the ML.FastTree but if you know of a workaround then that would be great, i.e. is there an obscure using reference that isn't automatically inserted by the model I need to insert.\n\n\n---\n#### Document Details\n\n⚠ *Do not edit this section. It is required for docs.microsoft.com ➟ GitHub issue linking.*\n\n* ID: 1b723be0-9b74-15da-b850-2b420c4a31b6\n* Version Independent ID: 557580d1-4af7-2fb7-9e48-643bcb14c11d\n* Content: [TreeExtensions.FastTreeTweedie Method (Microsoft.ML)](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.treeextensions.fasttreetweedie?view=ml-dotnet)\n* Content Source: [dotnet/xml/Microsoft.ML/TreeExtensions.xml](https://github.com/dotnet/ml-api-docs/blob/live/dotnet/xml/Microsoft.ML/TreeExtensions.xml)\n* Product: **dotnet-ml-api**\n* GitHub Login: @sfilipi\n* Microsoft Alias: **johalex**","Url":"https://github.com/dotnet/machinelearning/issues/4258","RelatedDescription":"Open issue \"Model Builder reference error\" (#4258)"},{"Id":"499450711","IsPullRequest":false,"CreatedAt":"2019-09-27T13:35:46","Actor":"fwaris","Number":"4257","RawContent":null,"Title":"What is the input format required by the LDA transform?","State":"open","Body":"Ideally I would like the output of ApplyWordEmbedding transform as input to LDA for topic modeling but apparently that is not the case.\r\n\r\nSo I am not sure how to format the input to LDA.\r\n\r\nDocumentation says it should be a vector of single - which it is with after applying the embedding transform but then I see an error message when running LDA:\r\n\r\nThe specified documents are all empty in column 'Features'.\r\n\r\nWhere \"Features\" is the output of the embedding.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/4257","RelatedDescription":"Open issue \"What is the input format required by the LDA transform?\" (#4257)"},{"Id":"499357959","IsPullRequest":false,"CreatedAt":"2019-09-27T10:08:43","Actor":"IgnasZ","Number":"4256","RawContent":null,"Title":"Incorrect constructor parameter type specified","State":"open","Body":"Constructor parameter 'columns' should be of type IEnumerable&lt;SchemaShape.Column&gt; instead of IEnumerable&lt;SchemaShape&gt;\n\n---\n#### Document Details\n\n⚠ *Do not edit this section. It is required for docs.microsoft.com ➟ GitHub issue linking.*\n\n* ID: 52902231-74e7-53a0-1974-ab96d7963da1\n* Version Independent ID: ebf148b4-3861-d68a-33ca-f12d59e6e3ee\n* Content: [SchemaShape Class (Microsoft.ML)](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.schemashape?view=ml-dotnet#feedback)\n* Content Source: [dotnet/xml/Microsoft.ML/SchemaShape.xml](https://github.com/dotnet/ml-api-docs/blob/live/dotnet/xml/Microsoft.ML/SchemaShape.xml)\n* Product: **dotnet-ml-api**\n* GitHub Login: @sfilipi\n* Microsoft Alias: **johalex**","Url":"https://github.com/dotnet/machinelearning/issues/4256","RelatedDescription":"Open issue \"Incorrect constructor parameter type specified\" (#4256)"},{"Id":"499166449","IsPullRequest":false,"CreatedAt":"2019-09-26T23:52:40","Actor":"gkapellmann","Number":"4254","RawContent":null,"Title":"An Autoencoder Sample","State":"open","Body":"This might not be an issue but more like a request.\r\n\r\nIs there a way to do an autoencoder? If so, can there be an example to achieve this?","Url":"https://github.com/dotnet/machinelearning/issues/4254","RelatedDescription":"Open issue \"An Autoencoder Sample\" (#4254)"},{"Id":"499119961","IsPullRequest":true,"CreatedAt":"2019-09-26T21:15:00","Actor":"LittleLittleCloud","Number":"4253","RawContent":null,"Title":"Image featurization","State":"open","Body":"It's just the same PR of the one on Justin's [repo](https://github.com/justinormont/machinelearning/pull/4)\r\n- [x] Pass image path\r\n- [x] Seperate ImageFeaturizing into four parts\r\n- [x] CodeGen","Url":"https://github.com/dotnet/machinelearning/pull/4253","RelatedDescription":"Open PR \"Image featurization\" (#4253)"},{"Id":"499028344","IsPullRequest":false,"CreatedAt":"2019-09-26T17:49:41","Actor":"aslotte","Number":"4251","RawContent":null,"Title":"Jupyter + ML.NET | Plotly","State":"open","Body":"This may not be the correct forum for this question, and if not, I'm happy to close it.\r\n\r\nPlotly created very large graphs, greatly slowing down the browser, especially compared to it's Python counterparts. I created a simple Scatter plot based on a dataset of 30 Mb, and Google Chrome almost completely came to a halt. I've created similar plots in Python equivalents for 500+ Mb of data with the browser running smoothly. \r\n\r\nThis means that XPlot.Plotly will not be a feasible option for any real-life datasets. Are there any other alternatives for the .NET community that we can recommend? Or should I reach out to Plotly to see if this is just a bug?","Url":"https://github.com/dotnet/machinelearning/issues/4251","RelatedDescription":"Open issue \"Jupyter + ML.NET | Plotly\" (#4251)"},{"Id":"499021022","IsPullRequest":false,"CreatedAt":"2019-09-26T17:32:48","Actor":"aslotte","Number":"4250","RawContent":null,"Title":"Jupyter + ML.NET | DataFrame | Identical to Pandas or more C# like?","State":"open","Body":"The DataFrame class currently bears a lot of resembles with the Python equivalent of Pandas. \r\n\r\nAs I don't have any API documentation in front of me, it's difficult to know the answer to this, so I thought I would ask, are there any efforts to make it more \"C# like\"?\r\n\r\nE.g., I would like to do the following for example:\r\n```\r\nvar allFraudulentRowsByTransactionDate = dataFrame.Where(x => x.IsFraud).OrderBy(y => y.TransactionDate);\r\n```\r\n\r\n**or** \r\n```\r\nvar first10Rows = dataFrame.Take(10);\r\nvar first10Percent = dataFrame.Take(dataFrame.Count() * 0.1);\r\n```\r\n**or** \r\n\r\n```\r\nvar amountColumnForOnlyFraudulentCases = dataFrame.Where(y => y.IsFraud).Select(x => x.Amount);\r\n```","Url":"https://github.com/dotnet/machinelearning/issues/4250","RelatedDescription":"Open issue \"Jupyter + ML.NET | DataFrame | Identical to Pandas or more C# like?\" (#4250)"},{"Id":"498994622","IsPullRequest":false,"CreatedAt":"2019-09-26T17:25:15","Actor":"aslotte","Number":"4249","RawContent":null,"Title":"Jupyter + ML.NET | DataFrame | Create a ToOutputFormat() function to output HTML table","State":"closed","Body":"Registering the format of a DataFrame is currently a large code block that takes away from the ML problem we're trying to solve in the Notebook. \r\n\r\nWhat about creating a ToString() or ToOutputFormat() method that outputs the default table format which I believe most people would like to have from start. ","Url":"https://github.com/dotnet/machinelearning/issues/4249","RelatedDescription":"Closed issue \"Jupyter + ML.NET | DataFrame | Create a ToOutputFormat() function to output HTML table\" (#4249)"},{"Id":"498574293","IsPullRequest":true,"CreatedAt":"2019-09-26T14:28:34","Actor":"eerhardt","Number":"4247","RawContent":null,"Title":"Fix NgramExtractingTransformer GetSlotNames to not allocate a new delegate on every invoke.","State":"closed","Body":"This was showing up as allocating a huge number of delegates for no reason.\r\n\r\nI removed the delegate all together and just pass the VBuffer in.","Url":"https://github.com/dotnet/machinelearning/pull/4247","RelatedDescription":"Closed or merged PR \"Fix NgramExtractingTransformer GetSlotNames to not allocate a new delegate on every invoke.\" (#4247)"},{"Id":"498520195","IsPullRequest":true,"CreatedAt":"2019-09-25T20:57:50","Actor":"LittleLittleCloud","Number":"4246","RawContent":null,"Title":"WIP - AutoML Add Recommendation Task","State":"open","Body":"What's already be done in this PR\r\n- [x] added `Recommendation` task and experiment in `AutoML`\r\n- [x] added `MatrixFactorization` as `MatrixFactorizationExtension`\r\n- [x] added a new Column Purpose (`LabelFeature`) and it's corresponding TransformerExtension (`LabelCategorical`) so that `AutoML` can construct the pre-process pipeline for `MatrixFactorizationExtension` correctly\r\n- [x] added a new recommendation example (with rating only) in `AutoML.Example`, and you can play with that!\r\n\r\nWhat's need to be done (Feel Free to CRUD)\r\n- [ ] figuring out how to accelerate and properly presenting the training process. Seems that `MatrixFactorization` requires more time to train a round, and the algorithm for sweeping params requires to train many rounds to find out the best parameter. It's time costy and customers might not like that.\r\n- [ ] Corresponding `CodeGen` part\r\n- [ ] Test case!\r\n- [ ] Better Naming and code style\r\n- [ ] Enable support for multiple feature trainers in `AutoML` (it requires some refactor works and shouldn't be done in this PR. But it's important)","Url":"https://github.com/dotnet/machinelearning/pull/4246","RelatedDescription":"Open PR \"WIP - AutoML Add Recommendation Task\" (#4246)"},{"Id":"497293022","IsPullRequest":true,"CreatedAt":"2019-09-23T19:48:34","Actor":"jamessantiago","Number":"4245","RawContent":null,"Title":"[Example Only] Anomaly detection example for extending AutoML features to non-experiment types","State":"open","Body":"We are excited to review your PR.\r\n\r\nSo we can do the best job, please check:\r\n\r\n- [x] There's a descriptive title that will make sense to other developers some time from now. \r\n- [x] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\r\n- [x] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\r\n- [x] You have included any necessary tests in the same PR.\r\n\r\nThis is for issue #4244 as an example.  Not sure the proper way to go about achieving the same result so this is just for discussion.","Url":"https://github.com/dotnet/machinelearning/pull/4245","RelatedDescription":"Open PR \"[Example Only] Anomaly detection example for extending AutoML features to non-experiment types\" (#4245)"}],"ResultType":"GitHubIssue"}},"RunOn":"2019-10-02T05:30:40.7110351Z","RunDurationInMilliseconds":601}