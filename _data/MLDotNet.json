{"Data":{"GitHub":{"Issues":[{"Id":"408972600","IsPullRequest":true,"CreatedAt":"2019-02-13T02:14:34","Actor":"rogancarr","Number":"2503","RawContent":null,"Title":"Add validation scenario tests","State":"closed","Body":"This PR adds a test for training with a validation set.\r\n\r\nFixes #2499","Url":"https://github.com/dotnet/machinelearning/pull/2503","RelatedDescription":"Closed or merged PR \"Add validation scenario tests\" (#2503)"},{"Id":"408946115","IsPullRequest":false,"CreatedAt":"2019-02-13T02:14:34","Actor":"rogancarr","Number":"2499","RawContent":null,"Title":"Create functional tests for all V1 validation scenarios","State":"closed","Body":"As laid out in #2498 , we need scenarios to cover the Validation functionality we want fully supported in V1.\r\n\r\n- Cross-validation (already covered)\r\n- Validation","Url":"https://github.com/dotnet/machinelearning/issues/2499","RelatedDescription":"Closed issue \"Create functional tests for all V1 validation scenarios\" (#2499)"},{"Id":"409582757","IsPullRequest":false,"CreatedAt":"2019-02-13T01:38:33","Actor":"artidoro","Number":"2527","RawContent":null,"Title":"Renaming .Train to .Fit in TrainerEstimators","State":"open","Body":"Some trainers allow training with an initial predictor, or with a validation data set. The initial predictor is used as a starting point for further training. However, the method that allows this is usually named:\r\n\r\n```csharp\r\nITransformer Train(IDataView trainData, IPredictor initialPredictor = null)\r\n```\r\nor \r\n```csharp\r\nITransformer Train(IDataView trainData, IDataView validationData = null, IPredictor initialPredictor = null)\r\n```\r\n\r\nThis could lead to quite a bit of confusion as the TrainerEstimators will have two methods for training -  `Fit()` and `.Train()` - that achieve pretty much the same result. In the above `initialPredictor` can be `null` so `.Fit(data)` does the same as `.Train(data)`.\r\n\r\nAs it was suggested in the issue #2502 in one of @TomFinley's comments https://github.com/dotnet/machinelearning/issues/2502#issuecomment-462493936, I think we should rename the `Train` method to `Fit`.\r\n\r\nThe result is that there will be a single method name `Fit` to train a model with an overload to perform training with an initial predictor. \r\n\r\n/cc: @TomFinley, @Ivanidzo4ka \r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2527","RelatedDescription":"Open issue \"Renaming .Train to .Fit in TrainerEstimators\" (#2527)"},{"Id":"409580785","IsPullRequest":true,"CreatedAt":"2019-02-13T01:29:24","Actor":"TomFinley","Number":"2526","RawContent":null,"Title":"Update calibrator estimators to be more suitable.","State":"open","Body":"Fixes #2515, contributes towards #1871 and indirectly towards #2251.\r\n\r\n* Internalize infrastructure only interface ICalibratorTrainer.\r\n* Update calibrator estimators so they are a suitable replacement for calibrator trainers in the public surface, e.g., no longer take IPredictor.\r\n\r\nNote that I did not add a calibrator catalog, since:\r\n\r\n1. I believe @sfilipi already has assigned herself the issue #1871 to do so, and\r\n2. Real reason, I am lazy! 😄\r\n\r\nCalibrator estimators now are configured by the score/label/optionally weight column, *except* for the fixed Platt estimator, which only takes score (since it is not trained).\r\n\r\nNote that ultimately the trainer estimator constructors *will* be internal, pending #1871.","Url":"https://github.com/dotnet/machinelearning/pull/2526","RelatedDescription":"Open PR \"Update calibrator estimators to be more suitable.\" (#2526)"},{"Id":"409576610","IsPullRequest":true,"CreatedAt":"2019-02-13T01:12:04","Actor":"wschin","Number":"2525","RawContent":null,"Title":"Fix an initial-value problem caused by unseen row/column","State":"open","Body":"Update LIBMF to have this [fix](https://github.com/cjlin1/libmf/pull/25). The problem is that when the largest column index in the training set doesn't match the number of columns in the specified matrix shape, columns' starting addresses may look like `[ptr_to_first_element_in_row_1, ptr_to_first_element_in_row_8, ptr_to_the_end_of_element_array, nullptr, nullptr]` but the correct value should be `[ptr_to_first_element_in_row_1, ptr_to_first_element_in_row_8, ptr_to_the_end_of_element_array, ptr_to_the_end_of_element_array, ptr_to_the_end_of_element_array]`.\r\n\r\nFixes https://github.com/dotnet/machinelearning/issues/2488","Url":"https://github.com/dotnet/machinelearning/pull/2525","RelatedDescription":"Open PR \"Fix an initial-value problem caused by unseen row/column\" (#2525)"},{"Id":"409575973","IsPullRequest":true,"CreatedAt":"2019-02-13T01:09:13","Actor":"abgoswam","Number":"2524","RawContent":null,"Title":"Store environment when creating new catalog entries ","State":"open","Body":"Fixes #2523 \r\n\r\nThe following changes are being made:\r\n\r\n1. Just _store_ the environment passed in , instead of creating subhosts\r\n2. Updated baselines to make tests pass\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/2524","RelatedDescription":"Open PR \"Store environment when creating new catalog entries \" (#2524)"},{"Id":"409574317","IsPullRequest":false,"CreatedAt":"2019-02-13T01:02:00","Actor":"abgoswam","Number":"2523","RawContent":null,"Title":"Registering subhosts when creating new catalog entries advances the pseudo random number generator","State":"open","Body":"For creating a new catalog entry in MLContext, the existing pattern is to register subhosts i.e. create a new Host out of the environment. This advances the pseudo random number generator. \r\n\r\nhttps://github.com/dotnet/machinelearning/blob/5c442a96edc3ac7a03bc10ef668310647f16db9a/src/Microsoft.ML.Data/TrainCatalog.cs#L195-L200\r\n\r\nThis means that if we add a new catalog entry in MLContext, the baseline tests would start failing.  (In fact this is the exact behaviour observed in PR #2390 when we try to add a AnomalyDetectionCatalog entry to MLContext)\r\n\r\nInstead of *forking* the environment we can just *store* the environment passed in.\r\n\r\n@TomFinley ","Url":"https://github.com/dotnet/machinelearning/issues/2523","RelatedDescription":"Open issue \"Registering subhosts when creating new catalog entries advances the pseudo random number generator\" (#2523)"},{"Id":"409568014","IsPullRequest":false,"CreatedAt":"2019-02-13T00:36:48","Actor":"shmoradims","Number":"2522","RawContent":null,"Title":"Docs and samples for the API reference site (P0 & P1 Trainers)","State":"open","Body":"List of Trainers:\r\n\r\n| BinaryClassification.Trainers | Category | Priority | Owner | Completed PR | \r\n| ------- | -------- | -------- | ------ | ----------- |\r\n| StochasticDualCoordinateAscent | Linear | 0 | Shahab | |\r\n| StochasticGradientDescent | Linear | 0 | Shahab | |\r\n| AveragedPerceptron | Linear | 0 | Shahab | |\r\n| LogisticRegression | Linear | 0 | Shahab | |\r\n| SymbolicStochasticGradientDescent | Linear | 0 | Shahab | |\r\n| FastTree | Tree | 0 |Shahab | |\r\n| FastForest | Tree | 0 | Shahab | |\r\n| LightGbm | Tree | 0 | Shahab | |\r\n| FieldAwareFactorizationMachine | FFM | 0 | Shahab |  |\r\n| GeneralizedAdditiveModels | GAM | 1 |Shahab | |\r\n| LinearSupportVectorMachines | Linear | 2 |  | |\r\n\r\n| Trainer | Category | Priority | Owner | Completed PR | \r\n| ------- | -------- | -------- | ------ | ----------- |\r\n| SDCAMC: Fast Linear Multi-class Classification (SA-SDCA) | Linear | 0 |  | |\r\n| SDCAR: Fast Linear Regression (SA-SDCA) | Linear | 0 |  | |\r\n| OVA: One-vs-All | Meta | 0 | | |\r\n| FastTreeRegression: FastTree (Boosted Trees) Regression | Tree | 0 |  | |\r\n| KMeansPlusPlus: KMeans++ Clustering | Clustering | 0 | | |\r\n| LightGBMMulticlass: LightGBM Multi-class Classifier | Tree | 0 | | |\r\n| LightGBMRegression: LightGBM Regressor | Tree | 0 | | |\r\n| MultiClassLogisticRegression: Multi-class Logistic Regression | Linear | 0 |  | |\r\n| OLSLinearRegression: Ordinary Least Squares (Regression) | Linear | 0 | | |\r\n| FastForestRegression: Fast Forest Regression | Tree | 0 | | |\r\n| RegressionGamTrainer: Generalized Additive Model for Regression | GAM | 1 | | |\r\n| OnlineGradientDescent: Stochastic Gradient Descent (Regression) | Linear | 1 |  | |\r\n| PoissonRegression: Poisson Regression | Linear | 1 |  | |\r\n| PKPD: Pairwise coupling (PKPD) | Meta | 1 | | |\r\n| pcaAnomaly: PCA Anomaly Detector | Projection | 1 |  | |\r\n| FastTreeTweedieRegression: FastTree (Boosted Trees) Tweedie Regression | Tree | 1 | | |\r\n| PriorPredictor: Prior Predictor | Baseline | 2 | artidoro  |#2510 |\r\n| RandomPredictor: Random Predictor | Baseline | 2 | artidoro   |#2510 |\r\n| MultiClassNaiveBayes: Multiclass Naive Bayes | Bayes | 2 | | |\r\n| BinarySGD: Hogwild SGD (binary) | Linear | 2 |  | |\r\n| FastTreeRanking: FastTree (Boosted Trees) Ranking | Tree | 2 | | |\r\n| LightGBMRanking: LightGBM Ranking | Tree | 2 | | |","Url":"https://github.com/dotnet/machinelearning/issues/2522","RelatedDescription":"Open issue \"Docs and samples for the API reference site (P0 & P1 Trainers)\" (#2522)"},{"Id":"409564953","IsPullRequest":false,"CreatedAt":"2019-02-13T00:24:22","Actor":"rogancarr","Number":"2521","RawContent":null,"Title":"FastTree EarlyStoppingMetric is an `int` but only accepts specific values","State":"open","Body":"In the current `Options` for `FastTree` (and `GAMs`), `EarlyStoppingMetric` is an `int`, but only specific values are accepted (e.g. 1 or 2 for regression, 1 or 3 for ranking).\r\n\r\nIt would be much cleaner if this was an enum.","Url":"https://github.com/dotnet/machinelearning/issues/2521","RelatedDescription":"Open issue \"FastTree EarlyStoppingMetric is an `int` but only accepts specific values\" (#2521)"},{"Id":"409563915","IsPullRequest":false,"CreatedAt":"2019-02-13T00:20:21","Actor":"rogancarr","Number":"2520","RawContent":null,"Title":"FastTree EarlyStoppingRule definition is inconsistent with API","State":"open","Body":"In `FastTree`, the `EarlyStoppingRule` is defined as\r\n```cs\r\n[Argument(ArgumentType.Multiple, HelpText = \"Early stopping rule. (Validation set (/valid) is required.)\", ShortName = \"esr\", NullName = \"<Disable>\")]\r\n[TGUI(Label = \"Early Stopping Rule\", Description = \"Early stopping rule. (Validation set (/valid) is required.)\")]\r\npublic IEarlyStoppingCriterionFactory EarlyStoppingRule;\r\n```\r\n\r\nThis can be specified like so:\r\n```cs\r\nvar fastTreeTrainer = mlContext.Regression.Trainers.FastTree(new \r\n    Trainers.FastTree.FastTreeRegressionTrainer.Options {\r\n        EarlyStoppingMetrics = 2,\r\n        EarlyStoppingRule = new GLEarlyStoppingCriterion.Arguments()\r\n    });\r\n```\r\n\r\nThis exposes the `IComponentFactory` way of doing things in the public API, which seems to be inconsistent with the public API. One suggestion is to use an `enum` over the existing options.\r\n\r\nThat said, this does support custom early stopping methods through implementing  `IEarlyStoppingCriterionFactory`.","Url":"https://github.com/dotnet/machinelearning/issues/2520","RelatedDescription":"Open issue \"FastTree EarlyStoppingRule definition is inconsistent with API\" (#2520)"},{"Id":"409561261","IsPullRequest":false,"CreatedAt":"2019-02-13T00:09:52","Actor":"Ivanidzo4ka","Number":"2519","RawContent":null,"Title":"OnlineLinear trainer InitialWeights option is not user friendly","State":"open","Body":"```\r\n        [Argument(ArgumentType.AtMostOnce, HelpText = \"Initial Weights and bias, comma-separated\", ShortName = \"initweights\")]\r\n        [TGUI(NoSweep = true)]\r\n        public string InitialWeights;\r\n```\r\nIt need to be replaced with two fields Bias and Weights and they should be float and array of floats instead of just string","Url":"https://github.com/dotnet/machinelearning/issues/2519","RelatedDescription":"Open issue \"OnlineLinear trainer InitialWeights option is not user friendly\" (#2519)"},{"Id":"409555869","IsPullRequest":true,"CreatedAt":"2019-02-12T23:50:45","Actor":"rogancarr","Number":"2518","RawContent":null,"Title":"Add Functional Tests for Data I/O","State":"open","Body":"This PR adds functional tests for Data I/O\r\n\r\n- Reading from IEnumerable\r\n- Writing to IEnumerable\r\n- Writing to and reading from delimited tex files\r\n  - With inferred schema\r\n  - With explicit schema\r\n- Writing to and reading from Binary files\r\n\r\nFixes #2508 ","Url":"https://github.com/dotnet/machinelearning/pull/2518","RelatedDescription":"Open PR \"Add Functional Tests for Data I/O\" (#2518)"},{"Id":"409555465","IsPullRequest":true,"CreatedAt":"2019-02-12T23:49:17","Actor":"shmoradims","Number":"2517","RawContent":null,"Title":"Documentation for BinaryClassification.AveragedPerceptron (V2)","State":"open","Body":"#2483 got messed up with a bad rebase. Recreating the PR here.\r\n\r\nDocs & sample for BinaryClassification.AveragedPerceptron. Related to #1209.","Url":"https://github.com/dotnet/machinelearning/pull/2517","RelatedDescription":"Open PR \"Documentation for BinaryClassification.AveragedPerceptron (V2)\" (#2517)"},{"Id":"409070226","IsPullRequest":true,"CreatedAt":"2019-02-12T23:36:53","Actor":"Ivanidzo4ka","Number":"2509","RawContent":null,"Title":"Internalize IDataTransform","State":"closed","Body":"Hide the unhideable, bestfriend the unfrienable!\r\nfixes https://github.com/dotnet/machinelearning/issues/1995","Url":"https://github.com/dotnet/machinelearning/pull/2509","RelatedDescription":"Closed or merged PR \"Internalize IDataTransform\" (#2509)"},{"Id":"408998651","IsPullRequest":true,"CreatedAt":"2019-02-12T21:53:58","Actor":"Ivanidzo4ka","Number":"2507","RawContent":null,"Title":"Get rid of value tuples in TrainTest and CrossValidation","State":"closed","Body":"Fixes #2501","Url":"https://github.com/dotnet/machinelearning/pull/2507","RelatedDescription":"Closed or merged PR \"Get rid of value tuples in TrainTest and CrossValidation\" (#2507)"},{"Id":"408949370","IsPullRequest":false,"CreatedAt":"2019-02-12T21:53:58","Actor":"rogancarr","Number":"2501","RawContent":null,"Title":"Remove value-tuples from the public surface of the API","State":"closed","Body":"The public surface of the API contains value-tuples, which present a few problems:\r\n- We can't change the data we return in future releases (e.g. add functionality)\r\n- They don't play well with F#\r\n- Support in VS isn't great\r\n\r\nThese occur are in at least\r\n- `CrossValidation` (e.g. for each task)\r\n- `TrainTestSplit`: (e.g. for each task)\r\n\r\nWe should make sure that no value-tuples are returned and that they are not in the parameters of any public API.\r\n\r\nRelated to: #2487 ","Url":"https://github.com/dotnet/machinelearning/issues/2501","RelatedDescription":"Closed issue \"Remove value-tuples from the public surface of the API\" (#2501)"},{"Id":"409483340","IsPullRequest":true,"CreatedAt":"2019-02-12T20:21:42","Actor":"Anipik","Number":"2516","RawContent":null,"Title":"Adding a new ubuntu CI leg","State":"open","Body":"Fixes https://github.com/dotnet/machinelearning/issues/2381\r\n\r\nCurrently we are just running the tests on centos linux. A couple of libraries dont run on centos distro but run on ubuntu so we are adding a new ci leg to increase coverafe and run these tests on ubuntu","Url":"https://github.com/dotnet/machinelearning/pull/2516","RelatedDescription":"Open PR \"Adding a new ubuntu CI leg\" (#2516)"},{"Id":"409468623","IsPullRequest":false,"CreatedAt":"2019-02-12T19:43:39","Actor":"TomFinley","Number":"2515","RawContent":null,"Title":"Calibrator estimators as successor to ICalibratorTrainer","State":"open","Body":"One of the legacy interfaces we have is the calibrator trainer.\r\n\r\nhttps://github.com/dotnet/machinelearning/blob/f269adcd30a33feeb969c17c2a9f9db235113cb1/src/Microsoft.ML.Data/Prediction/Calibrator.cs#L95\r\n\r\nThis interface is unsuitable to being a public class as part of the `IEstimator`/`ITransformer` idioms, being more of implementation interface for the infrastructure of the calibrators. So we don't want that exposed, but we do nonetheless want the ability to given some data, produce a calibrator out of it.\r\n\r\nHappily we *almost* have it, in the form of the subclasses of the calibrator estimators.\r\n\r\nhttps://github.com/dotnet/machinelearning/blob/f269adcd30a33feeb969c17c2a9f9db235113cb1/src/Microsoft.ML.Data/Prediction/CalibratorCatalog.cs#L52\r\n\r\nThe only real trouble is that this object assumes, at a fairly deep level, that internally there is an `IPredictor` in it, but as far as I know this is mostly due to some internal convenience code that I hope could be refactored to just take an `IDataView` directly.\r\n\r\nThis might be viewed as a sub-part of #1871.\r\n\r\n/ccing @sfilipi for visibility.","Url":"https://github.com/dotnet/machinelearning/issues/2515","RelatedDescription":"Open issue \"Calibrator estimators as successor to ICalibratorTrainer\" (#2515)"},{"Id":"409416767","IsPullRequest":false,"CreatedAt":"2019-02-12T17:32:36","Actor":"JRAlexander","Number":"2514","RawContent":null,"Title":"General Questions/observations","State":"open","Body":"All, I've cross posted [this](https://github.com/dotnet/docs/issues/10150) from dotnet/docs. Any insight or feedback you could give would be appreciated!  CC: @AlexSapple, @kkho (orginal posters)\r\n-----\r\nHi,\r\n\r\nI've started playing with ML.net and made a slightly modified version of [the taxi fare tutorial](https://docs.microsoft.com/en-gb/dotnet/machine-learning/tutorials/taxi-fare) to suit data I had. exercise has given me a few questions (please bear in mind I am completely new to this type of thing).\r\n\r\n1. My dataset was from elastic search documents - I found that I could use CreateStreamingDataView() to work with this. All the examples seem to use text file datasets. are text file datasets more common? (I come from a database world).\r\n2. working with int's caused errors when training a model. I was getting the following error: \"One or more errors occurred. (Column 'AdultCount' has values of I4which is not the same as earlier observed type of R4\".\r\n\r\nI think \"l4which\" was intended to be \"l4 which\" but in any case without any other changes, if i changed data type to be float, no such error. I'm not sure what R4 and L4 refer to but something to do with datatypes?\r\n\r\nwhen I got my modified example working - evaluating test data gave a metric of: RSquared = -0.12 and a RMS of 92.02. Clearly these values are outside the parameters that they should be - but I don't know how to troubleshoot such things - are there any tutorials on identifying why a model is messed up?\r\n\r\n1. I had a datetime field in my data. It may be a relevant field for making predictions (in my case I was looking at predicting a result count for a search with a 3rd party, based on existing logged search data). I am curious about fields like dates and how they might play a role - but not sure how they might be encoded into the pipeline. I thought that a date may be a lot more use if it was transformed into a day of year (or similar) to capture a pattern of seasonality rather than the pure date.\r\n2. my predictions were absolutely miles out (but that was expected after looking at the score of the model). I was in some cases predicting result counts of negative numbers.\r\n\r\noverall, I'm really excited by having the framework available to dev with - but really need more learning resources!\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2514","RelatedDescription":"Open issue \"General Questions/observations\" (#2514)"},{"Id":"409214400","IsPullRequest":false,"CreatedAt":"2019-02-12T10:01:31","Actor":"vivid216","Number":"2513","RawContent":null,"Title":"Load tensorflow model created by python error while 'Visual Studio Tools for AI' well","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: \r\n    windows 10\r\n    ml.net v0.10.0\r\n\r\n- **.NET Version (eg., dotnet --info)**: \r\n  dotcore v2.1\r\n\r\n### Issue\r\n\r\n- **What did you do?**\r\n- **What happened?**\r\n- **What did you expect?**\r\n\r\n### Source code / logs\r\n\r\nI have a tensorflow model, when I use Visual Studio Tools for AI, it auto generator code like this\r\n\r\n` \r\n    \r\n    private static List<long> serving_defaultFeaturesShapeForSingleInput = new List<long> { 1, 6, 8 };\r\n    private static List<string> serving_defaultInputNames = new List<string> { \"features\" };\r\n    private static List<string> serving_defaultOutputNames = new List<string> { \"predictions\" };\r\n    etc...\r\n    /// <summary>\r\n    /// Runs inference on abc model for a single input data.\r\n    /// </summary>\r\n    /// <param name=\"features\">From signature: input 1 with tensor name features; Shape of the input: { 1, 6, 8 }</param>\r\n    public IEnumerable<float> Serving_default(IEnumerable<float> features)\r\n    {\r\n        List<Tensor> result = manager.RunModel(\r\n            modelName,\r\n            int.MaxValue,\r\n            serving_defaultInputNames,\r\n            new List<Tensor> { new Tensor(features.ToList(), serving_defaultFeaturesShapeForSingleInput) },\r\n            serving_defaultOutputNames\r\n        );\r\n        List<float> r0 = new List<float>();\r\n        result[0].CopyTo(r0);\r\n        return r0;\r\n    }     \r\n  `\r\nand everything goes well, I run this instance and got a return. \r\n\r\nwhen I use the ML.Net v0.10.0 try a test the same tensorflow model :\r\n\r\n`\r\n     \r\npublic class TensorData\r\n    {\r\n        [VectorType(1, 4, 6)]\r\n        [ColumnName(\"lstm_1_input\")]\r\n        public float[] input { get; set; }\r\n    }\r\n\r\n    etc.....\r\n\r\n    var pipeline = mlContext.Transforms.ScoreTensorFlowModel(\r\n       tensorModel, new[] { \"lstm_1_input\" }, new[] { \"lstm_1_input\" });\r\n    var data = GetTensorData();\r\n    var idv = mlContext.Data.ReadFromEnumerable(data);\r\n    var trainedModel = pipeline.Fit(idv);\r\n    var predicted = trainedModel.Transform(idv);\r\n\r\n`\r\n\r\nit throw a exception: \r\n>System.InvalidOperationException:“Input shape mismatch: Input 'lstm_1_input' has shape [1, 4, 6], but input data is Vec<R4, 1, 4, 6>.”\r\n\r\n1、 how to caputure the inputColumnNames/outputColumnNames from a exists model ,  when Visual Studio Tools for AI use different input/output names ?\r\n2、how to construct a shape [1, 4, 6], when I change input  type as float[,,] ,> it throw a\r\n >System.InvalidOperationException:“Variable length input columns not supported”.\r\n\r\nso how I can correctly use the tensorflow model ? please give a example, thanks much.\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2513","RelatedDescription":"Open issue \"Load tensorflow model created by python error while 'Visual Studio Tools for AI' well\" (#2513)"},{"Id":"409126387","IsPullRequest":false,"CreatedAt":"2019-02-12T05:24:35","Actor":"rogancarr","Number":"2512","RawContent":null,"Title":"SaveAsText documentation doesn't address scope of separator argument","State":"open","Body":"`SaveAsText` in the `Data` catalog has a `separator` parameter, which is specified as a `char`. However, this throws an `ArgumentOutOfRangeException` if the `separator` is not a \"space, tab, comma, semicolon, or bar\".\r\n\r\nCurrently, the only way to discover this delimiter limitation is to try and fail.\r\n\r\nWe have a few options. We could allow any character to be used as a delimiter, we could add documentation to the parameter that says it must be one of these characters, or, if we really do want to scope to a limited set of characters, then we could add a compile-time check by changing this to an `enum`.","Url":"https://github.com/dotnet/machinelearning/issues/2512","RelatedDescription":"Open issue \"SaveAsText documentation doesn't address scope of separator argument\" (#2512)"},{"Id":"409103426","IsPullRequest":true,"CreatedAt":"2019-02-12T03:28:09","Actor":"codemzs","Number":"2511","RawContent":null,"Title":"Lockdown Microsoft.ML.FastTree public surface","State":"open","Body":"fixes #2266 \r\n\r\n**Reduces public API count from 1098 to 274.**\r\n\r\n| Before | After |   \r\n|:-:|:-:|\r\n| ![image](https://user-images.githubusercontent.com/1211949/52609628-bddb0200-2e32-11e9-9687-212f6d2db92c.png) | ![image](https://user-images.githubusercontent.com/1211949/52609436-29709f80-2e32-11e9-96ea-c5deb61782db.png) ||\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/2511","RelatedDescription":"Open PR \"Lockdown Microsoft.ML.FastTree public surface\" (#2511)"},{"Id":"409071847","IsPullRequest":true,"CreatedAt":"2019-02-12T01:44:01","Actor":"artidoro","Number":"2510","RawContent":null,"Title":"Creation of components through MLContext, internalization, and renaming","State":"open","Body":"Fixes #1798, #1758 \r\n\r\n1. In this PR I internalize the constructors of `DnnImageFeaturizerEstimator`, `PriorTrainer`, and `RandomTrainer`. This required to add catalog extensions for these three trainers. Internalizing these constructor closes issue #1798. \r\n2. I also rename a few instances of `Arguments` to `Options` which closes issue #1758.\r\n3. I internalized some fields across `ITransformers` and `IEstimators`.\r\n4. I also moved the `Options` class in TensorflowTransform to the estimator. ","Url":"https://github.com/dotnet/machinelearning/pull/2510","RelatedDescription":"Open PR \"Creation of components through MLContext, internalization, and renaming\" (#2510)"},{"Id":"409018745","IsPullRequest":false,"CreatedAt":"2019-02-11T23:07:43","Actor":"rogancarr","Number":"2508","RawContent":null,"Title":"Create functional tests for all V1 Data I/O scenarios","State":"open","Body":"As laid out in #2498 , we need scenarios to cover the Data Sources functionality we want fully supported in V1.\r\n\r\nWe need to be able to read and write from\r\n- IEnumerable\r\n- Delimited files\r\n- Binary (.idv) files","Url":"https://github.com/dotnet/machinelearning/issues/2508","RelatedDescription":"Open issue \"Create functional tests for all V1 Data I/O scenarios\" (#2508)"},{"Id":"408971935","IsPullRequest":false,"CreatedAt":"2019-02-11T22:29:47","Actor":"rogancarr","Number":"2502","RawContent":null,"Title":"Validation training isn't supported by the new API","State":"closed","Body":"I cannot find a way to train with a validation set in the new API, and I think there is some confusion as to how this would work, were it possible.\r\n\r\nTake this example:\r\n\r\n```cs\r\n// Create a pipeline to train on the sentiment data\r\nvar trainData = readTrainData();\r\nvar validData = readValidData();\r\n\r\nvar pipeline = mlContext.Transforms.SomTransform.()\r\n    .AppendCacheCheckpoint(mlContext) as IEstimator<ITransformer>;\r\nvar preprocessor = pipeline.Fit(trainData);\r\nvar preprocessedValidData = preprocessor.Transform(validData);\r\n\r\n// Train model with validation set.\r\n// There is no way below to specify a validation set for the learner\r\npipeline = pipeline.Append(mlContext.Regression.Trainers.FastTree(numTrees: 2));\r\n// Nor is there a way to specify a validation set in the Fit\r\nvar model = pipeline.Fit(trainData);\r\n```\r\n\r\nThis example uses `FastTree`, but the same problem exists for `GAM` and `FFM`, our other validation-set trainers: There is nowhere to specify a validation set.\r\n\r\nA second problem exists, namely: If we were to specify a validaiton set, would we specify the raw, unprocessed validation set (here `validData`) or would we specify the pre-transformed validation set (here, `preprocessedValidData`).","Url":"https://github.com/dotnet/machinelearning/issues/2502","RelatedDescription":"Closed issue \"Validation training isn't supported by the new API\" (#2502)"},{"Id":"408996817","IsPullRequest":true,"CreatedAt":"2019-02-11T22:01:24","Actor":"wschin","Number":"2506","RawContent":null,"Title":"Typed SDCA binary trainers","State":"open","Body":"Make `SdcaBinaryTrainer` strongly-typed according to the type it produces. The existing `SdcaBinaryTrainer` can produce either calibrated linear model or linear model as mentioned in #2469. To fix #2469, we move common functionalities used in both cases to `SdcaBinaryTrainerBase<T>` where `T` is `CalibratedModelParametersBase<LinearBinaryModelParameters, PlattCalibrator` for calibrated case and `LinearBinaryModelParameters` otherwise. Top-level APIs are changed accordingly. For dynamic/static APIs, we have 4 SDCA binary trainers for\r\n\r\n1. Calibrated linear model with simple arguments.\r\n2. Calibrated linear model with advanced options.\r\n3. Uncalibrated linear model with simple arguments.\r\n4. Uncalibrated linear model with advanced options.\r\n\r\nIn addition, because we don't like auto-calibration, the output schema in uncalibrated cases should not contain a probability column. This PR also fixes this in the two derived trainer classes' `ComputeSdcaBinaryClassifierSchemaShape`; the only case with a probability column generated is training logistic regression with Sdca. We also have two separated `SdcaCalibratedBinaryTrainer.Options` and `SdcaBinaryTrainer.Options` derived from `BinaryArgumentBase` and therefore entry-points are changed too.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/2506","RelatedDescription":"Open PR \"Typed SDCA binary trainers\" (#2506)"},{"Id":"408990471","IsPullRequest":true,"CreatedAt":"2019-02-11T21:44:04","Actor":"yaeldekel","Number":"2505","RawContent":null,"Title":"Move IModelCombiner to Microsoft.ML.Data","State":"open","Body":"We have an implementation of IModelCombiner in Microsoft.ML.FastTree. This PR is to move the interface to Microsoft.ML.Data so that Microsoft.ML.FastTree doesn't need to reference Microsoft.ML.Ensemble.\r\n\r\nThis PR also internalizes most of the classes in Microsoft.ML.Ensemble, based on issue #2268 .","Url":"https://github.com/dotnet/machinelearning/pull/2505","RelatedDescription":"Open PR \"Move IModelCombiner to Microsoft.ML.Data\" (#2505)"},{"Id":"408987057","IsPullRequest":true,"CreatedAt":"2019-02-11T21:35:07","Actor":"sfilipi","Number":"2504","RawContent":null,"Title":"Towards 1529: replacing the predicates with an IEnumerable on IRowToRowMapper.GetDependencies","State":"open","Body":"More work towards #1529. \r\n\r\nMarked the pr as still working on it, because there is one test failing: TestAndPredictoOnIris; double-checking the changes on the CompositeRowToRowMapper. \r\n\r\n\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/pull/2504","RelatedDescription":"Open PR \"Towards 1529: replacing the predicates with an IEnumerable on IRowToRowMapper.GetDependencies\" (#2504)"},{"Id":"408947750","IsPullRequest":false,"CreatedAt":"2019-02-11T19:50:58","Actor":"jignparm","Number":"2500","RawContent":null,"Title":"Enable OnnxTransform tests on Ubuntu, Disable on CentOS","State":"open","Body":"### System information\r\n\r\n- **OS version/distro**: Ubuntu 16.04\r\n- **.NET Version (eg., dotnet --info)**: 2.1\r\n\r\n### Issue\r\n\r\nOnnxTransform tests need to be activated on  new Ubuntu leg (when ready)\r\n\r\nOnnxTransform tests need to be deactivated on  CentOS leg (glibc version is not compatible).\r\n\r\nUpdate OnnxTransform to throw friendly error  message when running on incompatible Linux distro.\r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2500","RelatedDescription":"Open issue \"Enable OnnxTransform tests on Ubuntu, Disable on CentOS\" (#2500)"},{"Id":"408941928","IsPullRequest":false,"CreatedAt":"2019-02-11T19:35:40","Actor":"rogancarr","Number":"2498","RawContent":null,"Title":"V1 Scenarios need to be covered by tests","State":"open","Body":"In issue #584, we laid out a set of scenarios that we'd like to cover for V1.0 of ML.NET. We need high-level functional tests to make sure that these work well in the 1.0 library.\r\n\r\nHere is a list of tests that cover the scenarios. Let's use this issue as a top-level issue to track coverage of the APIs.\r\n\r\nCategory | Scenarios | Link to Test | Completed PR | Blocked by Issue\r\n-- | -- | -- | -- | --\r\nData I/O| I can   use objects already in memory (as IEnumerable) as input to my ML   pipeline/experiment |   |   |  \r\nData I/O | I can   use locally stored delimited files (.csv, .tsv, etc.) as input to my ML   pipeline/experiment |   |   |  \r\nData I/O | I can   use locally stored binary files (.idv) as input to my ML   pipeline/experiment |   |   |  \r\nData I/O | I can   go through any arbitrary data transformation / model training and save the   output to disk as a delimited file (.csv, .tsv, etc.). |   |   |  \r\nData I/O | I can   go through any arbitrary data transformation / model training and save the   output to disk as a binary file (.idv). |   |   |  \r\nData I/O | I can   go through any arbitrary data transformation / model training and convert the   output to an IEnumerable. |   |   |  \r\n~~Data   Sources~~ | ~~I can   use data from a SQL database by reading it into memory or to disk using an   existing SQL reader and then use that as input to my ML pipeline/experiment~~ (Covered by delimited and enumerable use cases.) |   |   |  \r\nData   Transformation, Feature Engineering | I can   take an existing ONNX model and get predictions from it (as both final output   and as input to downstream pipelines) |   |   |  \r\nData   Transformation, Feature Engineering | Extensible   transformation: It should be possible to write simple row-mapping transforms.       Examples:   \"I can add custom steps to my pipeline such as creating a new column   that is the addition of two other columns, or easily add cosine similarity,   without having to create my own build of ML.NET.\" |   |   |  \r\nData   Transformation, Feature Engineering | I can   modify settings in the TextFeaturizer to update the number of word-grams and   char-grams used along with things like the normalization. |   |   |  \r\nData   Transformation, Feature Engineering | I can   apply normalization to the columns of my data |   |   |  \r\nData   Transformation, Feature Engineering | I can   take an existing TF model and get predictions from it or any layer in the   model |   |   |  \r\nData   Transformation and Feature Engineering | P1: I   can take an existing TF model and use ML.NET APIs to identify the input and   output nodes |   |   |  \r\nDebugging | I can see how each transform affects my data. An example   from Tom: E.g.: if I were to have the text \"Help I'm a   bug!\" I   should be able to see the steps where it is normalized to \"help i'm a bug\" then tokenized into [\"help\",   \"i'm\", \"a\", \"bug\"] then mapped into term   numbers [203,   25, 3, 511] then   projected into the sparse float vector {3:1, 25:1, 203:1, 511:1}, etc. etc. |   |   |  \r\nDebugging | I can   see how my data was read in to verify that I specified the schema correctly |   |   |  \r\nDebugging | I can   see the output at the end of my pipeline to see which columns are available   (score, probability, predicted label) |   |   |  \r\nDebugging | I can   look at intermediate steps of the pipeline to debug my model.       Example:   > I   were to have the text `\"Help I'm a bug!\"` I should be able to see   the steps where it is normalized to `\"help i'm a bug\"` then   tokenized into `[\"help\", \"i'm\", \"a\",   \"bug\"]` then mapped into term numbers `[203, 25, 3, 511]` then   projected into the sparse float vector `{3:1, 25:1, 203:1, 511:1}`, etc. etc. |   |   |  \r\nDebugging | P1: I   can access the information needed for understanding the progress of my   training (e.g. number of trees trained so far out of how many) |   |   |  \r\nEvaluation | I can   get predictions (scores, probabilities, predicted labels) for every row in a   test dataset |   |   |  \r\nEvaluation | I can   evaluate a model trained for any of my tasks on test data. The evaluation   outputs metrics that are relevant to the task (e.g. AUC, accuracy, P/R, and   F1 for binary classification) |   |   |  \r\nEvaluation | I can   reconfigure the threshold of my binary classification model based on analysis   of the PR curves or other metrics scores. | [Link](https://github.com/dotnet/machinelearning/blob/master/test/Microsoft.ML.Functional.Tests/Prediction.cs) |   |  #2465\r\nEvaluation | (Might   not work?)   I can   map the score/probability for each class to the original class labels I   provided in the pipeline (multiclass, binary classification). |   |   |  \r\nEvaluation | P1: I   can get the data that will allow me to plot PR curves |   |   |  \r\nExplainability   & Interpretability | I can   get near-free (local) feature importance for scored examples (Feature   Contributions) |   |   |  \r\nExplainability   & Interpretability | I can   view how much each feature contributed to each prediction for trees and   linear models (Feature Contributions) |   |   |  \r\nExplainability   & Interpretability | I can   view the overall importance of each feature (Permutation Feature Importance,   GetFeatureWeights) |   |   |  \r\nExplainability   & Interpretability | I can   train interpretable models (linear model, GAM) |   |   |  \r\nIntrospective   training | I can   take an existing model file and inspect what transformers were included in   the pipeline |   |   |  \r\nIntrospective   training | I can   inspect the coefficients (weights and bias) of a linear model without much   work. Easy to find via auto-complete. |   |   |  \r\nIntrospective   training | I can   inspect the normalization coefficients of a normalizer in my pipeline without   much work. Easy to find via auto-complete. |   |   |  \r\nIntrospective   training | I can   inspect the trees of a boosted decision tree model without much work. Easy to   find via auto-complete. |   |   |  \r\nIntrospective   training | I can   inspect the topics after training an LDA transform. Easy to find via   auto-complete. |   |   |  \r\nIntrospective   training | I can   inspect a categorical transform and see which feature values map to which key   values. Easy to find via auto-complete. |   |   |  \r\nIntrospective   training | P1: I   can access the GAM feature histograms through APIs |   |   |  \r\nModel   files | I can   train a model and save it as a file. This model includes the learner as well   as the transforms   (e.g.   Decomposability) |   |   |  \r\nModel   files | I can   use a model file in a completely different process to make predictions.    (e.g.   Decomposability) |   |   |  \r\nModel   files | I can   use newer versions of ML.NET with ML.NET model files of previous versions   (for v1.x) |   |   |  \r\nModel   files | I can   easily figure out which NuGets (and versions) I need to score an ML.NET model |   |   |  \r\nModel   files | P2: I   can move data between NimbusML and ML.NET (using IDV). Prepare with NimbusML   and load with ML.NET |   |   |  \r\nModel   files | P2: I   can use model files interchangeably between compatible versions of ML.NET and   NimbusML. |   |   |  \r\nSaving   data | P1: I   can export ML.NET models to ONNX (limited to the existing internal   functionality) |   |   |  \r\nSaving   Data | I can   save a model to text |   |   |  \r\nTasks | I can   train a model to do classification (binary and multiclass) |   |   |  \r\nTasks | I can   train a model to do regression |   |   |  \r\nTasks | I can   train a model to do anomaly detection |   |   |  \r\nTasks | I can   train a model to do recommendations |   |   |  \r\nTasks | I can   train a model to do ranking |   |   |  \r\nTasks | I can   train a model to do clustering |   |   |  \r\nTraining | I can   provide multiple learners and easily compare evaluation metrics between them. |   |   |  \r\nTraining | I can   use an initial predictor to update/train the model for some trainers (e.g.   linear learners like averaged perceptron). Specifically, start the weights   for the model from the existing weights. |   |   |  \r\nTraining | Metacomponents   smartly restrict their use to compatible components.       Example:   \"When specifying what trainer OVA should use, a user will be able to   specify any binary classifier. If they specify a regression or multi-class   classifier ideally that should be a compile error.\" |   |   |  \r\nTraining | I can   train TF models when I bring a TF model topology |   |   |  \r\nTraining | I can   use OVA and easily add any binary classifier to it |   |   |  \r\nUse in   web environments | I can   use ML.NET models to make predictions in multi-threaded environments like   ASP.NET. (This doesn't have to be inherent in the prediction engine but   should be easy to do.) |   |   |  \r\nValidation | Cross-validation:   I can take a pipeline and easily do cross validation on it without having to   know how CV works.       Have a   mechanism to do cross validation, that is, you come up with a data source   (optionally with stratification column), come up with an instantiable   transform and trainer pipeline, and it will handle (1) splitting up the data,   (2) training the separate pipelines on in-fold data, (3) scoring on the   out-fold data, (4) returning the set of evaluations and optionally trained   pipes. (People always want metrics out of xfold, they _sometimes_ want the   actual models too.) | [Link](https://github.com/dotnet/machinelearning/blob/master/test/Microsoft.ML.Functional.Tests/Validation.cs)  |  #2470 |  \r\nValidation | I can   use a validation set in a pipeline for learners that support them (e.g.   FastTree, GAM) |   [Link](https://github.com/dotnet/machinelearning/blob/master/test/Microsoft.ML.Functional.Tests/Validation.cs) | #2503 |  \r\n\r\n","Url":"https://github.com/dotnet/machinelearning/issues/2498","RelatedDescription":"Open issue \"V1 Scenarios need to be covered by tests\" (#2498)"}],"ResultType":"GitHubIssue"}},"RunOn":"2019-02-13T05:30:48.9535326Z","RunDurationInMilliseconds":829}